{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "496631ea",
      "metadata": {
        "id": "496631ea"
      },
      "source": [
        "# Módulo 1: Análisis de datos en el ecosistema Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a14cd3f",
      "metadata": {
        "id": "3a14cd3f"
      },
      "source": [
        "### Sesión (22)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e08fed2",
      "metadata": {
        "id": "2e08fed2"
      },
      "source": [
        "## Distribution based clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7110c9ac",
      "metadata": {
        "id": "7110c9ac"
      },
      "source": [
        "El agrupamiento basado en distribución es un tipo de algoritmo de clustering que asume que los puntos de datos en un cluster o grupo **se generan a partir de una distribución de probabilidad**. Esto quiere decir que los datos del mismo grupo **provienen de la misma distribución estadística subyacente**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae130af",
      "metadata": {
        "id": "7ae130af"
      },
      "source": [
        "![dist.jpg](attachment:dist.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa62475e",
      "metadata": {
        "id": "fa62475e"
      },
      "source": [
        "### Gaussian Mixture Models (GMM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad3ee2d",
      "metadata": {
        "id": "aad3ee2d"
      },
      "source": [
        "El tipo más común de agrupamiento basado en distribución son los **modelos de mezcla gaussiana** o *Gaussian Mixture Models*. El ***GMM*** es un modelo probabilístico que representa **una mezcla de distribuciones gaussianas**, cada una de las cuales corresponde a un grupo diferente. El modelo estima los parámetros de las distribuciones gaussianas que mejor se ajustan a los datos y asigna cada punto de datos al grupo que maximiza su probabilidad de pertenecer a ese grupo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5662aa5",
      "metadata": {
        "id": "b5662aa5"
      },
      "source": [
        "![gmm.png](attachment:gmm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2149cf2f",
      "metadata": {
        "id": "2149cf2f"
      },
      "source": [
        "El modelo ***GMM*** tiene como objetivo encontrar los parámetros de cada distribución gaussiana, dadao el número de distribuciones óptimas. Los pasos aplicar el algoritmo GMM son los siguientes:\n",
        "\n",
        "- **Inicialización**: Se selecciona el **número de distribuciones** gaussianas y **se inicializan las medias y las varianzas** de cada distribución.\n",
        "\n",
        "- **Expectativa**: Se calcule la **probabilidad** de que cada punto de datos pertenezca a cada distribución gaussiana.\n",
        "\n",
        "- **Maximización**: Se vuelven a **estimar las medias, las covarianzas y los coeficientes de mezcla** utilizando las probabilidades calculadas en el paso anterior.\n",
        "\n",
        "- **Convergencia**: Se repiten los pasos de _Expectativa_ y _Maximización_ hasta que la probabilidad logarítmica de los datos **ya no se aumente significativamente**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6211cc9",
      "metadata": {
        "id": "b6211cc9"
      },
      "source": [
        "![density_estimation.gif](attachment:density_estimation.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6ec3a8",
      "metadata": {
        "id": "fb6ec3a8"
      },
      "source": [
        "A pesar de que los modelos de tipo `GaussianMixture` de la librería _sklearn_ requieren **saber a periori el número de los clusters** al igual que el algoritmo _KMeans_, disponen de un método propio llamado `bic()` para calcular el criterio de información Bayesiana que **facilita el hecho de encontrar el número óptimo** de los clusters presentes en el conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc64c1f",
      "metadata": {
        "id": "fbc64c1f"
      },
      "source": [
        "Ahora amos a crear un set de datos con un **número considerable de clústeres** y una **varianza mayor que la unidad**, con el fin de comprobar diferentes métricas de rendimiento de clustering y agrupar los datos mediante el algoritmo _GMM_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a59ece",
      "metadata": {
        "id": "44a59ece"
      },
      "outputs": [],
      "source": [
        "# importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be1883e",
      "metadata": {
        "id": "0be1883e"
      },
      "outputs": [],
      "source": [
        "# Modificamos los parámetros de los gráficos en matplotlib\n",
        "from matplotlib.pyplot import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 18, 8 # el primer dígito es el ancho y el segundo el alto\n",
        "rcParams[\"font.weight\"] = \"bold\"\n",
        "rcParams[\"font.size\"] = 10\n",
        "rcParams[\"axes.labelweight\"] = \"bold\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e634a16",
      "metadata": {
        "id": "4e634a16"
      },
      "outputs": [],
      "source": [
        "# Generar el datset sintético\n",
        "from sklearn.datasets import make_blobs\n",
        "datos_sint, etiquetas_sint, centroides_sint = make_blobs(n_samples=1000, centers=8, cluster_std=1.5, return_centers=True, random_state=10)\n",
        "sns.scatterplot(x=datos_sint[:,0], y=datos_sint[:,1])\n",
        "plt.title(\"Datos sintéticos sin etiquetas\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10f16ab",
      "metadata": {
        "id": "c10f16ab"
      },
      "source": [
        "Como se puede ver, el conjunto de datos contiene varios grupos que tampoco son fáciles de distinguir de forma visual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df32274f",
      "metadata": {
        "id": "df32274f"
      },
      "outputs": [],
      "source": [
        "# Pintar los datos según el cluster y con sus centros\n",
        "sns.scatterplot(x=datos_sint[:,0], y=datos_sint[:,1], hue=etiquetas_sint)\n",
        "sns.scatterplot(x=centroides_sint[:,0], y=centroides_sint[:,1], color='green', s=90, label='Centroides')\n",
        "plt.title(\"Datos sintéticos etiquetados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d897f0",
      "metadata": {
        "id": "03d897f0"
      },
      "source": [
        "Estandarizamos los datos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c87eea3",
      "metadata": {
        "id": "4c87eea3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado_sint= StandardScaler().fit(datos_sint)\n",
        "datos_sint_norm = escalado_sint.transform(datos_sint)\n",
        "datos_sint_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4470c54",
      "metadata": {
        "id": "e4470c54"
      },
      "source": [
        "Empezamos a **analizar distintos criterios** que hemos estudiado hasta ahora con el objetivo de encontrar el número de grupos en los datos, para realizar un ejercicio de **aprendizaje no-supervisado**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519c1063",
      "metadata": {
        "id": "519c1063"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "distor_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    distor_clust.append(modelo_km.inertia_)\n",
        "\n",
        "# Obtener una visualización más elaborada\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=distor_clust, color='green', label='WSS versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Suma de la distancia al cuadrado de cada punto a su centroide\", fontsize=16)\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"WSS\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ce07dc",
      "metadata": {
        "id": "c6ce07dc"
      },
      "outputs": [],
      "source": [
        "sil_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    sil_clust.append(silhouette_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=sil_clust, color='green', label='silueta versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Distancia media dentro del grupo (a) entre la distancia media del grupo más cercano (b)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"silhouette_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c51f24",
      "metadata": {
        "id": "19c51f24"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "dav_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    dav_clust.append(davies_bouldin_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=dav_clust, color='green', label='the Davies-Bouldin score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Davies-Bouldin index\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"davies_bouldin_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eb157e",
      "metadata": {
        "id": "40eb157e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "calinsk_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    calinsk_clust.append(calinski_harabasz_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=calinsk_clust, color='green', label='Calinski and Harabasz score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Variance Ratio Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"calinski_harabasz_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5427e53b",
      "metadata": {
        "id": "5427e53b"
      },
      "source": [
        "Los criterios de información bayesianos (***BIC***) son una medida del ajuste del modelo que miden de alguna manera el **equilibrio** entre la **complejidad del modelo** y la **calidad** del ajuste (_goodness of fit_).\n",
        "\n",
        "- cuando el **número de clusters** es demasiado **pequeño**, el BIC será **grande** debido a la **deficiencia del modelo**.\n",
        "- cuando el **número de clústeres es demasiado grande**, el BIC también será **grande** debido a la **mayor complejidad del modelo**.\n",
        "\n",
        "Así que **el número óptimo de clusters** es el que **minimiza el _BIC_**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0568224b",
      "metadata": {
        "id": "0568224b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def bic_score(X, labels):\n",
        "  \"\"\"\n",
        "  BIC score for the penalization of complexity-vs-goodness of fit of clusters.\n",
        "  \"\"\"\n",
        "\n",
        "  n_points = len(labels)\n",
        "  n_clusters = len(set(labels))\n",
        "  n_dimensions = X.shape[1]\n",
        "\n",
        "  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n",
        "\n",
        "  loglikelihood = 0\n",
        "  for label_name in set(labels):\n",
        "    X_cluster = X[labels == label_name]\n",
        "    n_points_cluster = len(X_cluster)\n",
        "    centroid = np.mean(X_cluster, axis=0)\n",
        "    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n",
        "    loglikelihood += \\\n",
        "      n_points_cluster * np.log(n_points_cluster) \\\n",
        "      - n_points_cluster * np.log(n_points) \\\n",
        "      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n",
        "      - (n_points_cluster - 1) / 2\n",
        "\n",
        "  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n",
        "\n",
        "  return -bic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543702e3",
      "metadata": {
        "id": "543702e3"
      },
      "outputs": [],
      "source": [
        "bic_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    bic_clust.append(bic_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_clust, color='green', label='BIC score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Bayesian Information Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1e5c5d",
      "metadata": {
        "id": "ab1e5c5d"
      },
      "source": [
        "Ahora graficamos la evolución del valor de _BIC_ que se mide para los modelos tipo _GMM_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d54362",
      "metadata": {
        "id": "36d54362"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "bic_gmm = []\n",
        "for k in range(2, 15):\n",
        "    modelo_gmm = GaussianMixture(n_components=k, random_state=100)\n",
        "    modelo_gmm.fit(datos_sint_norm)\n",
        "    bic_gmm.append(modelo_gmm.bic(datos_sint_norm))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_gmm, color='green', label='BIC score of GMM Model versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Bayesian Information Criterion (GMM-based)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score (GMM)\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af935792",
      "metadata": {
        "id": "af935792"
      },
      "source": [
        "Comprobamos si el algoritmo **Mrean Shift** es capaz de intuir el número de los grupos originales que están presentes en el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194323b3",
      "metadata": {
        "id": "194323b3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "modelo_ms_sint = MeanShift(bin_seeding=True)\n",
        "\n",
        "modelo_ms_sint.fit(datos_sint_norm)\n",
        "\n",
        "y_etiquetas_ms_sint = modelo_ms_sint.labels_\n",
        "\n",
        "centros_ms_sint = modelo_ms_sint.cluster_centers_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_sint_norm, y_etiquetas_ms_sint))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_sint_norm[:,0], y=datos_sint_norm[:,1], hue=y_etiquetas_ms_sint)\n",
        "sns.scatterplot(x=centros_ms_sint[:,0], y=centros_ms_sint[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con Mean Shift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a98d650",
      "metadata": {
        "id": "5a98d650"
      },
      "source": [
        "Un vez que hayamos estimado el número de los clusters en el dataset, podemos usar el método `GaussianMixture` para crear el algoritmo de clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959863fc",
      "metadata": {
        "id": "959863fc"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "model_gmm = GaussianMixture(n_components=8, random_state=100)\n",
        "\n",
        "model_gmm.fit(datos_sint_norm)\n",
        "\n",
        "y_etiquetas_gmm = model_gmm.predict(datos_sint_norm)\n",
        "\n",
        "centros_clust_gmm = model_gmm.means_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_sint_norm, y_etiquetas_gmm))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_sint_norm[:,0], y=datos_sint_norm[:,1], hue=y_etiquetas_gmm)\n",
        "sns.scatterplot(x=centros_clust_gmm[:,0], y=centros_clust_gmm[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con GMM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9029c41f",
      "metadata": {
        "id": "9029c41f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568da658",
      "metadata": {
        "id": "568da658"
      },
      "source": [
        "### Ejemplo con datos heteroscedásticos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407bd8d4",
      "metadata": {
        "id": "407bd8d4"
      },
      "source": [
        "En estadística, la **homocedasticidad** se refiere a la propiedad de **tener varianzas iguales** en todos los grupos o subpoblaciones, mientras que la **heterocedasticidad** se refiere a la propiedad de **tener varianzas desiguales** en todos los grupos o clustres presentes en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8952887f",
      "metadata": {
        "id": "8952887f"
      },
      "outputs": [],
      "source": [
        "datos2, etiquetas2, centros2 = make_blobs(n_samples=1000, centers=3, cluster_std=[1, 2.5, 0.5],\n",
        "                                                        return_centers=True, random_state=88)\n",
        "\n",
        "\n",
        "sns.scatterplot(x=datos2[:,0], y=datos2[:,1], hue=etiquetas2)\n",
        "sns.scatterplot(x=centros2[:,0], y=centros2[:,1], color='green', s=90, label='Centroides')\n",
        "plt.title(\"Datos sintéticos heteroscedásticos\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1feff7",
      "metadata": {
        "id": "8d1feff7"
      },
      "outputs": [],
      "source": [
        "# Normalizar los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado2 = StandardScaler().fit(datos2)\n",
        "datos2_norm = escalado2.transform(datos2)\n",
        "datos2_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c4ccb3",
      "metadata": {
        "id": "70c4ccb3"
      },
      "outputs": [],
      "source": [
        "# Verificar las características de los valores estandarizados\n",
        "display(pd.DataFrame(datos2).describe().round(2))\n",
        "display(pd.DataFrame(datos2_norm).describe().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bffce82",
      "metadata": {
        "id": "9bffce82"
      },
      "source": [
        "Después de normalizar los datos miramos si somos capaces de estimar el número óptimo de los clustres en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e0fc82",
      "metadata": {
        "id": "44e0fc82"
      },
      "outputs": [],
      "source": [
        "distor_clust = []\n",
        "for k in range(1, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos2_norm)\n",
        "    distor_clust.append(modelo_km.inertia_)\n",
        "\n",
        "# Obtener una visualización más elaborada\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(1,15), y=distor_clust, color='green', label='SSE versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Suma de la distancia al cuadrado de cada punto a su centroide\", fontsize=16)\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"SSE\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35370cf",
      "metadata": {
        "id": "c35370cf"
      },
      "outputs": [],
      "source": [
        "sil_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos2_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos2_norm)\n",
        "    sil_clust.append(silhouette_score(datos2_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=sil_clust, color='green', label='silueta versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Distancia media dentro del grupo (a) entre la distancia media del grupo más cercano (b)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"silhouette_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee2c695",
      "metadata": {
        "id": "aee2c695"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "dav_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos2_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos2_norm)\n",
        "    dav_clust.append(davies_bouldin_score(datos2_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=dav_clust, color='green', label='the Davies-Bouldin score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Davies-Bouldin index\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"davies_bouldin_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f804c721",
      "metadata": {
        "id": "f804c721"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "calinsk_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos2_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos2_norm)\n",
        "    calinsk_clust.append(calinski_harabasz_score(datos2_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=calinsk_clust, color='green', label='Calinski and Harabasz score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Variance Ratio Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"calinski_harabasz_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d1512e",
      "metadata": {
        "id": "08d1512e"
      },
      "outputs": [],
      "source": [
        "bic_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos2_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos2_norm)\n",
        "    bic_clust.append(bic_score(datos2_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_clust, color='green', label='BIC score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Bayesian Information Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884be132",
      "metadata": {
        "id": "884be132"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "bic_gmm = []\n",
        "for k in range(2, 15):\n",
        "    modelo_gmm = GaussianMixture(n_components=k, random_state=100)\n",
        "    modelo_gmm.fit(datos2_norm)\n",
        "    bic_gmm.append(modelo_gmm.bic(datos2_norm))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_gmm, color='green', label='BIC score of GMM Model versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Bayesian Information Criterion (GMM-based)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d9f5bc",
      "metadata": {
        "id": "84d9f5bc"
      },
      "source": [
        "Procedemos también a crear un modelo de clustering usando la técnica de ***Mean Shift*** que no requiere conocer el hiperparametro de `K` a periori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750746fc",
      "metadata": {
        "id": "750746fc"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "modelo_ms = MeanShift(bin_seeding=True)\n",
        "\n",
        "modelo_ms.fit(datos2_norm)\n",
        "\n",
        "y_etiquetas_ms = modelo_ms.labels_\n",
        "\n",
        "centros2_ms = modelo_ms.cluster_centers_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos2_norm, y_etiquetas_ms))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_etiquetas_ms)\n",
        "sns.scatterplot(x=centros2_ms[:,0], y=centros2_ms[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con Mean Shift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6d9526",
      "metadata": {
        "id": "fe6d9526"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=etiquetas2, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos con etiquetas\")\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_etiquetas_ms, ax=axes[1])\n",
        "axes[1].set_title(\"Datos etiquetados por clustering (Mean Shift)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145e6e70",
      "metadata": {
        "id": "145e6e70"
      },
      "outputs": [],
      "source": [
        "diff_pos_ms = [i for i in range(len(y_etiquetas_ms)) if y_etiquetas_ms[i]!=etiquetas2[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_ms))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_ms)/len(y_etiquetas_ms))*100))\n",
        "y_etiquetas_ms[diff_pos_ms]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53008e43",
      "metadata": {
        "id": "53008e43"
      },
      "source": [
        "Vemos que en este caso y para un conjunto de datos heteroscedásticos, los métodos de **Davies-Bouldin** y **BIC** de **GMM** han podido calcular correctamente el número de los clústeres."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6af197",
      "metadata": {
        "id": "3a6af197"
      },
      "source": [
        "Ahora desarrollamos un algoritmo de _K-Means clustering_ suponiendo conocer previamente el número de los grupos existentes en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825674ac",
      "metadata": {
        "id": "825674ac"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "modelo_km2 = KMeans(n_clusters=3, random_state=100)\n",
        "\n",
        "modelo_km2.fit(datos2_norm)\n",
        "\n",
        "y_km2 = modelo_km2.labels_\n",
        "\n",
        "centros2_km = modelo_km2.cluster_centers_\n",
        "\n",
        "print(\"SSE = \", modelo_km2.inertia_)\n",
        "print(\"Silhouette score = \", silhouette_score(datos2_norm, y_km2))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_km2)\n",
        "sns.scatterplot(x=centros2_km[:,0], y=centros2_km[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con K-means\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d22e405",
      "metadata": {
        "id": "5d22e405"
      },
      "outputs": [],
      "source": [
        "y_label_km2 = np.where(y_km2==0, 1, np.where(y_km2==1, 2, 0))\n",
        "y_label_km2 = pd.Series(y_label_km2, name='label')\n",
        "y_label_km2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ddad53",
      "metadata": {
        "id": "82ddad53"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=etiquetas2, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_label_km2, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (K-Means)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8361d9b0",
      "metadata": {
        "id": "8361d9b0"
      },
      "outputs": [],
      "source": [
        "diff_pos_km2 = [i for i in range(len(y_label_km2)) if y_label_km2[i]!=etiquetas2[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_km2))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_km2)/len(y_label_km2))*100))\n",
        "y_label_km2[diff_pos_km2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c42920",
      "metadata": {
        "id": "97c42920"
      },
      "source": [
        "Ahora creamos un modelo tipo _GMM_ con los datos heteroscedásticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fa5906",
      "metadata": {
        "id": "d9fa5906"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "model_gmm2 = GaussianMixture(n_components=3, random_state=100)\n",
        "\n",
        "model_gmm2.fit(datos2_norm)\n",
        "\n",
        "y_etiquetas_gmm2 = model_gmm2.predict(datos2_norm)\n",
        "\n",
        "centros2_gmm = model_gmm2.means_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos2_norm, y_etiquetas_gmm2))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_etiquetas_gmm2)\n",
        "sns.scatterplot(x=centros2_gmm[:,0], y=centros2_gmm[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con GMM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331a82fa",
      "metadata": {
        "id": "331a82fa"
      },
      "outputs": [],
      "source": [
        "y_label_gmm2 = np.where(y_etiquetas_gmm2==0, 1, np.where(y_etiquetas_gmm2==1, 2, 0))\n",
        "y_label_gmm2 = pd.Series(y_label_gmm2, name='label')\n",
        "y_label_gmm2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2505cc",
      "metadata": {
        "id": "9b2505cc"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=etiquetas2, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_label_gmm2, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (GMM)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae5cd537",
      "metadata": {
        "id": "ae5cd537"
      },
      "outputs": [],
      "source": [
        "diff_pos_gmm2 = [i for i in range(len(y_label_gmm2)) if y_label_gmm2[i]!=etiquetas2[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_gmm2))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_gmm2)/len(y_label_gmm2))*100))\n",
        "y_label_gmm2[diff_pos_gmm2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773f7003",
      "metadata": {
        "id": "773f7003"
      },
      "source": [
        "Como podemos observar el modelo de _GMM_ ha conseguido **mejores resultados** en este caso y para los **datos heteroscedásticos**, por intentar a **buscar las distribuciones de probabilidad subyacentes** de cada grupo en vez de aplicar un enfoque basado en centroides como el algoritmo _KMeans_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f998dc3f",
      "metadata": {
        "id": "f998dc3f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c72793",
      "metadata": {
        "id": "b3c72793"
      },
      "source": [
        "## Hierarchical clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d2cc75",
      "metadata": {
        "id": "d3d2cc75"
      },
      "source": [
        "El **agrupamiento jerárquico** es un tipo de algoritmo de clustering que crea una jerarquía de grupos **dividiendo recursivamente** los datos en **grupos más pequeños** hasta que **cada punto de datos esté en su propio grupo**. Esta jerarquía generalmente se representa como un ***dendograma***, un diagrama en **forma de árbol** que muestra el orden y la jerarquía de los grupos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38443a7c",
      "metadata": {
        "id": "38443a7c"
      },
      "source": [
        "![hclust.png](attachment:hclust.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78038b18",
      "metadata": {
        "id": "78038b18"
      },
      "source": [
        "\n",
        "Hay dos tipos principales de ***hierarchical clustering***:\n",
        "- **Aglomerativo**: Cada dato comienza como su propio grupo y el **algoritmo fusiona gradualmente pares de grupos** en función de su similitud hasta que todos los puntos de datos estén en **un solo grupo**.\n",
        "\n",
        "- **Divisivo**:  Todos los puntos de datos comienzan en un solo grupo y el **algoritmo divide el grupo en grupos más pequeños** en función de su diferencia, hasta que cada dato esté en **su propio grupo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0bef1a",
      "metadata": {
        "id": "0c0bef1a"
      },
      "source": [
        "Un aspecto importante del agrupamiento jerárquico es la elección del **método de vinculación (_linkage method_)** y la **métrica de distancia (_distance metric_)** utilizada para medir la similitud entre los grupos como `Euclidean`, `Manhattan` o `Minkowski`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4410b4",
      "metadata": {
        "id": "db4410b4"
      },
      "source": [
        "![distance.png](attachment:distance.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde036ff",
      "metadata": {
        "id": "bde036ff"
      },
      "source": [
        "\n",
        "El método de vinculación o ***linkage method*** determina **cómo se calcula la distancia entre dos clústeres** en función de las distancias entre los puntos de datos de sus miembros. Los métodos de vinculación más comunes son:\n",
        "\n",
        "- **Centroid**: la distancia entre los centros de clusters.\n",
        "- **Single**: la distancia más corta entre dos puntos de datos.\n",
        "- **Complete**: la distancia más larga entre dos puntos.\n",
        "- **Average**: la distancia promedio entre todos los pares de puntos de datos.\n",
        "- **Ward**: la suma de las distancias al cuadrado desde el centro del nuevo cluster, dentro de todos los grupos que se fusionan."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9054d3",
      "metadata": {
        "id": "5f9054d3"
      },
      "source": [
        "![linkage.png](attachment:linkage.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaaea224",
      "metadata": {
        "id": "eaaea224"
      },
      "source": [
        "**No** existe un **criterio único** como otras medidas de rendimiento de clustering para calcular el número óptimo de grupos a partir de un **dendograma**. Sin embargo, puede resultar muy útil dibujar este gráfico sobre todo cuando no hay una cantidad ingente de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a391bd",
      "metadata": {
        "id": "e4a391bd"
      },
      "source": [
        "![Dendrogram.png](attachment:Dendrogram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac9bc63",
      "metadata": {
        "id": "9ac9bc63"
      },
      "source": [
        "En resumen, el **eje vertical** de un dendrograma representa el **nivel de disimilitud o diferencia** entre los objetos o variables que se agrupan, mientras que el **eje horizontal** muestra los **diferentes grupos** que se forman en cada nivel del árbol y la **distancia o diferencia entre ellos**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b35241f",
      "metadata": {
        "id": "1b35241f"
      },
      "source": [
        "Para crear el dendograma debemos usar la librería ___SciPy___ aunque después para crear el algoritmo de clustering aglomorativo podemos usar la librería _sklearn_ como el resto de los modelos. Vamos a realizar un ejercicio sobre el conjunto de datos heteroscedásticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34abf83",
      "metadata": {
        "id": "d34abf83"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Realizar el método \"agglomerative clustering\" con la vinculación tipo \"linkage\"\n",
        "linkage_method = linkage(datos2_norm, method ='ward', metric='euclidean')\n",
        "\n",
        "# Visualizar el dendograma y la posible línea para cortar el árbol jerarquico\n",
        "Dendrogram = dendrogram(linkage_method)\n",
        "plt.axhline(23, color='red', linestyle='--');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886fc916",
      "metadata": {
        "id": "886fc916"
      },
      "source": [
        "Considerando el número de los grupos que forman el datset, procedemos a crear el modelo final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dcbfa1",
      "metadata": {
        "id": "a6dcbfa1"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "modelo_agg = AgglomerativeClustering(n_clusters=3, linkage='ward',affinity='euclidean')\n",
        "\n",
        "modelo_agg.fit(datos2_norm)\n",
        "\n",
        "y_etiquetas_agg = modelo_agg.labels_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos2_norm, y_etiquetas_agg))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_etiquetas_agg)\n",
        "plt.title(\"Agglomerative Clustering\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76fe37b7",
      "metadata": {
        "id": "76fe37b7"
      },
      "outputs": [],
      "source": [
        "y_label_agg = np.where(y_etiquetas_agg==0, 1, np.where(y_etiquetas_agg==1, 0, 2))\n",
        "y_label_agg = pd.Series(y_label_agg, name='label')\n",
        "y_label_agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b6732c",
      "metadata": {
        "id": "88b6732c"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=etiquetas2, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos2_norm[:,0], y=datos2_norm[:,1], hue=y_label_agg, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (Agglomerative)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1440285",
      "metadata": {
        "id": "f1440285"
      },
      "outputs": [],
      "source": [
        "diff_pos_agg = [i for i in range(len(y_label_agg)) if y_label_agg[i]!=etiquetas2[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_agg))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_agg)/len(y_label_agg))*100))\n",
        "y_label_agg[diff_pos_agg]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab5357c",
      "metadata": {
        "id": "bab5357c"
      },
      "source": [
        "Podemos observar que el método de _agrupamiento jerárquico (aglomerativo)_ ha llegado a agrupar los datos heteroscedásticos con una buena precisión sobre todo comparando con el método _KMeans_.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2919f588",
      "metadata": {
        "id": "2919f588"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5964526d",
      "metadata": {
        "id": "5964526d"
      },
      "source": [
        "## Density based clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c17e62",
      "metadata": {
        "id": "85c17e62"
      },
      "source": [
        "El **agrupamiento basado en densidad** (_Density-based clustering_) es una técnica de clustering que identifica grupos de puntos de datos **en función de la densidad local** de puntos. La principal idea de este método es que los clústeres son **regiones de alta densidad** separadas por regiones de baja densidad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7027ba",
      "metadata": {
        "id": "3e7027ba"
      },
      "source": [
        "![density.png](attachment:density.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa997d9d",
      "metadata": {
        "id": "fa997d9d"
      },
      "source": [
        "\n",
        "\n",
        " El algoritmo de _agrupación espacial de aplicaciones con ruido basada en la densidad_ o **DBSCAN (_Density-Based Spatial Clustering of Applications with Noise_)**. En _DBSCAN_, los puntos que se encuentran en regiones de alta densidad se denominan como **puntos centrales** (_core points_) y los puntos que se encuentran en regiones de baja densidad pero que aún forman parte de un clúster se denominan **puntos de borde** (_border points_). Los puntos que no están en ningún grupo se denominan **puntos de ruido** (_noise points_)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0f058b",
      "metadata": {
        "id": "bf0f058b"
      },
      "source": [
        "![dbscan.gif](attachment:dbscan.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cf402a",
      "metadata": {
        "id": "e2cf402a"
      },
      "source": [
        "El algoritmo _DBSCAN_ toma dos parámetros principales como entrada:\n",
        "- **epsilon ($ε$)**: Representa el radio a considerar alrededor de cada punto.\n",
        "- **min_samples ($minPts$)**: Representa el número mínimo de puntos necesarios para formar una región densa."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb35389",
      "metadata": {
        "id": "dcb35389"
      },
      "source": [
        "![dbscan.png](attachment:dbscan.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfe8ed7",
      "metadata": {
        "id": "adfe8ed7"
      },
      "source": [
        "**OPTICS (_Ordering Points To Identify the Clustering Structure_)** es un algoritmo de clustering basado en la densidad que es similar a _DBSCAN_. Este modelo produce un gráfico de accesibilidad (_reachability plot_), que es una representación gráfica de la estructura de agrupamiento de datos y puede manejar conjuntos de datos con **densidades variables**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd286bd",
      "metadata": {
        "id": "edd286bd"
      },
      "source": [
        "Ahora realizamos un ejercicio usando los datos con formas que no sean esféricas:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4626e4ce",
      "metadata": {
        "id": "4626e4ce"
      },
      "source": [
        "### Datasets con patrones irregulares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fd836f",
      "metadata": {
        "id": "49fd836f"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "datos_luna, etiquetas_luna = make_moons(n_samples=1000, noise=0.05, random_state=77)\n",
        "\n",
        "sns.scatterplot(x=datos_luna[:,0], y=datos_luna[:,1])\n",
        "plt.title(\"Datos sintéticos lunares\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b7c8bf1",
      "metadata": {
        "id": "1b7c8bf1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "datos_luna, etiquetas_luna = make_moons(n_samples=1000, noise=0.05, random_state=77)\n",
        "\n",
        "sns.scatterplot(x=datos_luna[:,0], y=datos_luna[:,1], hue=etiquetas_luna)\n",
        "plt.title(\"Datos sintéticos lunares\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbdaf24",
      "metadata": {
        "id": "7dbdaf24"
      },
      "outputs": [],
      "source": [
        "# Normalizar los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado_luna = StandardScaler().fit(datos_luna)\n",
        "datos_luna_norm = escalado_luna.transform(datos_luna)\n",
        "datos_luna_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee9a84c",
      "metadata": {
        "id": "9ee9a84c"
      },
      "outputs": [],
      "source": [
        "# Verificar las características de los valores estandarizados\n",
        "display(pd.DataFrame(datos_luna).describe().round(2))\n",
        "display(pd.DataFrame(datos_luna_norm).describe().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e944d4e1",
      "metadata": {
        "id": "e944d4e1"
      },
      "source": [
        "Analizamos los diferentes métodos para ver si se puede descubrir el número de los clusters a periori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b44f6dc",
      "metadata": {
        "id": "5b44f6dc"
      },
      "outputs": [],
      "source": [
        "distor_clust = []\n",
        "for k in range(1, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_luna_norm)\n",
        "    distor_clust.append(modelo_km.inertia_)\n",
        "\n",
        "# Obtener una visualización más elaborada\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(1,15), y=distor_clust, color='green', label='WSS versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Suma de la distancia al cuadrado de cada punto a su centroide\", fontsize=16)\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"WSS (inertia)\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8663f7a4",
      "metadata": {
        "id": "8663f7a4"
      },
      "outputs": [],
      "source": [
        "sil_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_luna_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_luna_norm)\n",
        "    sil_clust.append(silhouette_score(datos_luna_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=sil_clust, color='green', label='silueta versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Distancia media dentro del grupo (a) entre la distancia media del grupo más cercano (b)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"silhouette_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9125efbf",
      "metadata": {
        "id": "9125efbf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "dav_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_luna_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_luna_norm)\n",
        "    dav_clust.append(davies_bouldin_score(datos_luna_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=dav_clust, color='green', label='the Davies-Bouldin score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Davies-Bouldin index\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"davies_bouldin_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359e85be",
      "metadata": {
        "id": "359e85be"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "calinsk_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_luna_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_luna_norm)\n",
        "    calinsk_clust.append(calinski_harabasz_score(datos_luna_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=calinsk_clust, color='green', label='Calinski and Harabasz score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Variance Ratio Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"calinski_harabasz_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b79889",
      "metadata": {
        "id": "25b79889"
      },
      "outputs": [],
      "source": [
        "bic_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_luna_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_luna_norm)\n",
        "    bic_clust.append(bic_score(datos_luna_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_clust, color='green', label='BIC score versus K', linewidth=3)\n",
        "plt.xticks(range(1,15))\n",
        "plt.title(\"Bayesian Information Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6098eb02",
      "metadata": {
        "id": "6098eb02"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "bic_gmm = []\n",
        "for k in range(2, 15):\n",
        "    modelo_gmm = GaussianMixture(n_components=k, random_state=100)\n",
        "    modelo_gmm.fit(datos_luna_norm)\n",
        "    bic_gmm.append(modelo_gmm.bic(datos_luna_norm))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_gmm, color='green', label='BIC score of GMM Model versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Bayesian Information Criterion (GMM-based)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54ee82b",
      "metadata": {
        "id": "c54ee82b"
      },
      "source": [
        "Nínguno de los criterios han sido suficientemente eficaz para calcular el número de los grupos presentes en este conjunto de datos. Probamos la creación de un modelo _Mean Shift_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c357420",
      "metadata": {
        "id": "2c357420"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "modelo_ms_luna = MeanShift(bin_seeding=True)\n",
        "\n",
        "modelo_ms_luna.fit(datos_luna_norm)\n",
        "\n",
        "y_etiquetas_ms_luna = modelo_ms_luna.labels_\n",
        "\n",
        "centros_ms_luna = modelo_ms_luna.cluster_centers_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_luna_norm, y_etiquetas_ms_luna))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_ms_luna)\n",
        "sns.scatterplot(x=centros_ms_luna[:,0], y=centros_ms_luna[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con Mean Shift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80a2728",
      "metadata": {
        "id": "b80a2728"
      },
      "outputs": [],
      "source": [
        "y_label_ms_luna= np.where(y_etiquetas_ms_luna==0, 1, 0)\n",
        "y_label_ms_luna = pd.Series(y_label_ms_luna, name='label')\n",
        "y_label_ms_luna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7800a881",
      "metadata": {
        "id": "7800a881"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=etiquetas_luna, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos con etiquetas\")\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_label_ms_luna, ax=axes[1])\n",
        "axes[1].set_title(\"Datos etiquetados por clustering (Mean Shift)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf01b036",
      "metadata": {
        "id": "cf01b036"
      },
      "outputs": [],
      "source": [
        "diff_pos_ms_luna = [i for i in range(len(y_label_ms_luna)) if y_label_ms_luna[i]!=etiquetas_luna[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_ms_luna))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_ms_luna)/len(y_label_ms_luna))*100))\n",
        "y_label_ms_luna[diff_pos_ms_luna]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0348a334",
      "metadata": {
        "id": "0348a334"
      },
      "source": [
        "Ahora desarrollamos un algoritmo de _K-Means clustering_ suponiendo conocer previamente el número de los grupos existentes en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48500ef",
      "metadata": {
        "id": "b48500ef"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelo_km_luna = KMeans(n_clusters=2, random_state=100)\n",
        "\n",
        "modelo_km_luna.fit(datos_luna_norm)\n",
        "\n",
        "y_km_luna = modelo_km_luna.labels_\n",
        "\n",
        "centros_km_luna = modelo_km_luna.cluster_centers_\n",
        "\n",
        "print(\"SSE = \", modelo_km_luna.inertia_)\n",
        "print(\"Silhouette score = \", silhouette_score(datos_luna_norm, y_km_luna))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_km_luna)\n",
        "sns.scatterplot(x=centros_km_luna[:,0], y=centros_km_luna[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con K-means\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e80ebf",
      "metadata": {
        "id": "24e80ebf"
      },
      "outputs": [],
      "source": [
        "y_label_km_luna = np.where(y_km_luna==0, 1, 0)\n",
        "y_label_km_luna = pd.Series(y_label_km_luna, name='label')\n",
        "y_label_km_luna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ef5719",
      "metadata": {
        "id": "a4ef5719"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=etiquetas_luna, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_label_km_luna, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (K-Means)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c93b76f",
      "metadata": {
        "id": "8c93b76f"
      },
      "outputs": [],
      "source": [
        "diff_pos_km_luna = [i for i in range(len(y_label_km_luna)) if y_label_km_luna[i]!=etiquetas_luna[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_km_luna))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_km_luna)/len(y_label_km_luna))*100))\n",
        "y_label_km_luna[diff_pos_km_luna]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7fd881",
      "metadata": {
        "id": "bb7fd881"
      },
      "source": [
        "Vamos a comprobar los algoritmos de agrupamiento basados en la densidad local de puntos de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4f085b",
      "metadata": {
        "id": "8d4f085b"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "modelo_dbscan = DBSCAN()\n",
        "\n",
        "modelo_dbscan.fit(datos_luna_norm)\n",
        "\n",
        "y_etiquetas_dbscan = modelo_dbscan.labels_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_luna_norm, y_etiquetas_dbscan))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_dbscan)\n",
        "plt.title(\"DBSCAN - Density-Based Spatial Clustering of Applications with Noise\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c6b789",
      "metadata": {
        "id": "e8c6b789"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=etiquetas_luna, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos con etiquetas\")\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_dbscan, ax=axes[1])\n",
        "axes[1].set_title(\"Datos etiquetados por clustering (DBSCAN)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460d1274",
      "metadata": {
        "id": "460d1274"
      },
      "outputs": [],
      "source": [
        "diff_pos_dbscan_luna = [i for i in range(len(y_etiquetas_dbscan)) if y_etiquetas_dbscan[i]!=etiquetas_luna[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_dbscan_luna))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_dbscan_luna)/len(y_etiquetas_dbscan))*100))\n",
        "y_etiquetas_dbscan[diff_pos_dbscan_luna]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b6c413",
      "metadata": {
        "id": "17b6c413"
      },
      "source": [
        "Vemos que el resultado es ideal en caso del modelo _DBSCAN_. Probamos generar un modelo con otro tipo de algoritmos similares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f49d63",
      "metadata": {
        "id": "07f49d63"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "modelo_optics = OPTICS()\n",
        "\n",
        "modelo_optics.fit(datos_luna_norm)\n",
        "\n",
        "y_etiquetas_optics = modelo_optics.labels_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_luna_norm, y_etiquetas_optics))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_optics)\n",
        "plt.title(\"OPTICS - Ordering Points To Identify the Clustering Structure\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4baa7af",
      "metadata": {
        "id": "c4baa7af"
      },
      "source": [
        "Esta vez ajustamos los parámetros del modelo _OPTICS_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8794c368",
      "metadata": {
        "id": "8794c368"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "modelo_optics = OPTICS(min_samples=7, xi=0.1, min_cluster_size=0.2)\n",
        "\n",
        "modelo_optics.fit(datos_luna_norm)\n",
        "\n",
        "y_etiquetas_optics = modelo_optics.labels_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_luna_norm, y_etiquetas_optics))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_optics)\n",
        "plt.title(\"OPTICS - Ordering Points To Identify the Clustering Structure\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78315981",
      "metadata": {
        "id": "78315981"
      },
      "source": [
        "Como se puede observar, el rendimiento del modelo _OPTICS_ a diferencia del modelo _DBSCAN_ depende en gran medida de la elección de **parámetros** como las **muestras mínimas (*min_samples*)** y el **tamaño mínimo de los clusters (*min_cluster_size*)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb1eac0",
      "metadata": {
        "id": "2eb1eac0"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=etiquetas_luna, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos con etiquetas\")\n",
        "\n",
        "sns.scatterplot(x=datos_luna_norm[:,0], y=datos_luna_norm[:,1], hue=y_etiquetas_optics, ax=axes[1])\n",
        "axes[1].set_title(\"Datos etiquetados por clustering (OPTICS)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f801f45f",
      "metadata": {
        "id": "f801f45f"
      },
      "outputs": [],
      "source": [
        "diff_pos_optics_luna = [i for i in range(len(y_etiquetas_optics)) if y_etiquetas_optics[i]!=etiquetas_luna[i]]\n",
        "print(\"El modelo de clustering se ha equivocado en clasificar %s puntos\" % len(diff_pos_optics_luna))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_optics_luna)/len(y_etiquetas_optics))*100))\n",
        "y_etiquetas_optics[diff_pos_optics_luna]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ff51d1",
      "metadata": {
        "id": "41ff51d1"
      },
      "source": [
        "Estos puntos que se han etiquetado como $-1$ se consideran como **ruidos** en el ejercicio de agrupamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf5449b",
      "metadata": {
        "id": "faf5449b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbea03fe",
      "metadata": {
        "id": "cbea03fe"
      },
      "source": [
        "### Datasets con patrones no lineales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ac41ed",
      "metadata": {
        "id": "02ac41ed"
      },
      "source": [
        "Vamos a realizar un ejercicio de cluestring con datos irregulares que presentan una no linealidad alta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cfa571",
      "metadata": {
        "id": "f1cfa571"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "datos_cir, etiquetas_cir = make_circles(n_samples=1000, factor=0.5, noise=0.05, random_state=99)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(x=datos_cir[:,0], y=datos_cir[:,1])\n",
        "plt.title(\"Datos sintéticos circulares\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "231489e5",
      "metadata": {
        "id": "231489e5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "datos_cir, etiquetas_cir = make_circles(n_samples=1000, factor=0.5, noise=0.05, random_state=99)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(x=datos_cir[:,0], y=datos_cir[:,1], hue=etiquetas_cir)\n",
        "plt.title(\"Datos sintéticos circulares\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90cd589b",
      "metadata": {
        "id": "90cd589b"
      },
      "source": [
        "Aquí se puede ver que no hay una necesidad importante para normalizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39587596",
      "metadata": {
        "id": "39587596"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(datos_cir).describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a35d3f",
      "metadata": {
        "id": "86a35d3f"
      },
      "source": [
        "### **`Ejercicio 22.1`**\n",
        "\n",
        "**`22.1.1`** Utiliza las distintas técnicas de análisis de calidad de clustering para ver si se puede estimar a periori el número de los grupos de datos presentes en este data set:\n",
        "\n",
        " - WSS (Elbow method)\n",
        " - Silhouette score\n",
        " - Davies-Bouldin index\n",
        " - Calinski and Harabasz score\n",
        " - BIC score\n",
        " - BIC score of GMM\n",
        " - `random_state=100`\n",
        "\n",
        "**`22.1.2`** Crea diferentes modelos de clustering, genera las gráficas de cada modelo, dibuja la comparación con el dataset original y calcula sus **porcentages de aciertos** y compara los resultados obtenidos:  \n",
        "\n",
        " - Mean Shift\n",
        " - K-means\n",
        " - GMM\n",
        " - Agglomerative (con dendograma)\n",
        " - DBSCAN (`eps=0.25`)\n",
        " - OPTICS (`min_samples=7, xi=0.1, min_cluster_size=0.2`)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}