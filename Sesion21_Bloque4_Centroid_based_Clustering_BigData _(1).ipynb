{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "496631ea",
      "metadata": {
        "id": "496631ea"
      },
      "source": [
        "# Módulo 1: Análisis de datos en el ecosistema Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a14cd3f",
      "metadata": {
        "id": "3a14cd3f"
      },
      "source": [
        "### Sesión (21)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e08fed2",
      "metadata": {
        "id": "2e08fed2"
      },
      "source": [
        "## Centroid-based Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb17e2a",
      "metadata": {
        "id": "6bb17e2a"
      },
      "source": [
        "La agrupación en clústeres basada en el centroide (**[Centroid-based clustering](https://en.wikipedia.org/wiki/Cluster_analysis#Centroid-based_clustering:~:text=Centroid%2Dbased%20clustering%5Bedit%5D)**) es un tipo de clustering que divide un conjunto de datos en grupos o clusters en función de la similitud de sus puntos de datos, por eso pertenece al grupo de algoritmos basados en particiones (**partition-based algorithms**). En este método, se calcula un **centroide** para cada grupo, que representa el **centro** o el **promedio** de los puntos de datos dentro del grupo, que **no necesariamente es parte del conjunto de datos**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e35c68",
      "metadata": {
        "id": "08e35c68"
      },
      "source": [
        "![centroid1.png](attachment:centroid1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6deb8d9",
      "metadata": {
        "id": "f6deb8d9"
      },
      "source": [
        "Para diferenciar bien el **promedio** con  o el **punto intermedio** o la **mediana** en un conjunto de datos nos fijamos en los siguientes definiciones y ejemplos:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20dcb15",
      "metadata": {
        "id": "a20dcb15"
      },
      "source": [
        "### Mean vs. Median"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c521e5b6",
      "metadata": {
        "id": "c521e5b6"
      },
      "source": [
        "- **Mean/Average ([media aritmética](https://dle.rae.es/medio#:~:text=en%20sent.%20fig.-,media%20aritm%C3%A9tica,-1.%20f))**: Resultado de dividir la suma de varias cantidades por el número de ellas.\n",
        "- **Median ([mediana](https://dle.rae.es/mediano#:~:text=lado%20opuesto.-,9.%20f.%20Mat.,-Elemento%20de%20una))**: Elemento de una serie ordenada de valores crecientes de forma que la **divide en dos partes iguales**, superiores e inferiores a él."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a20769",
      "metadata": {
        "id": "e9a20769"
      },
      "source": [
        "La **mediana** se puede calcular de la siguiente menera:\n",
        "- Si el **número de observaciones es impar**, el número del **medio de la lista** ordenada es la mediana.\n",
        "- Si el **número de observaciones es par**, entonces la mediana es el **promedio simple de los dos números del medio** de la lista ordenada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bb3b16",
      "metadata": {
        "id": "12bb3b16"
      },
      "outputs": [],
      "source": [
        "# importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac9270b",
      "metadata": {
        "id": "2ac9270b"
      },
      "outputs": [],
      "source": [
        "# Modificamos los parámetros de los gráficos en matplotlib\n",
        "from matplotlib.pyplot import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 18, 8 # el primer dígito es el ancho y el segundo el alto\n",
        "rcParams[\"font.weight\"] = \"bold\"\n",
        "rcParams[\"font.size\"] = 10\n",
        "rcParams[\"axes.labelweight\"] = \"bold\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43b8084",
      "metadata": {
        "id": "c43b8084"
      },
      "outputs": [],
      "source": [
        "# Cuando el número de observaciones es impar\n",
        "A = [1,2,10,20,37]\n",
        "\n",
        "print(A)\n",
        "print(\"La media es igual a : \", pd.Series(A).mean())\n",
        "print(\"La mediana es igual a : \", pd.Series(A).median())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3baaac77",
      "metadata": {
        "id": "3baaac77"
      },
      "outputs": [],
      "source": [
        "# Cuando el número de observaciones es par\n",
        "B = [1,2,10,20,37,62]\n",
        "\n",
        "print(A)\n",
        "print(\"La media es igual a : \", pd.Series(B).mean())\n",
        "print(\"La mediana es igual a : \", pd.Series(B).median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4778aab3",
      "metadata": {
        "id": "4778aab3"
      },
      "source": [
        "La **mediana** en general es un valor mucho más **estable** comparando con la **media** de una variable, en caso de tener **alta dispersión** y en la presencia de **valores atípicos (outliers)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66063e4a",
      "metadata": {
        "id": "66063e4a"
      },
      "outputs": [],
      "source": [
        "datos = pd.DataFrame({'V1': [1,2,3,4,5,6,7],\n",
        "                      'V2': [1,2,3,4,5,6,70000]\n",
        "                      })\n",
        "display(datos)\n",
        "display(datos.describe())\n",
        "print(\"La media o el valor promedio para V2 es igual a:  \", datos['V2'].mean())\n",
        "print(\"La mediana o el valor mediano para V2 es igual a:  \", datos['V2'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4961707",
      "metadata": {
        "id": "f4961707"
      },
      "source": [
        "### K-Medoids Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837fab50",
      "metadata": {
        "id": "837fab50"
      },
      "source": [
        "La agrupación en clústeres de **[K-Medoids](https://en.wikipedia.org/wiki/K-medoids)** es un algoritmo de clustering basado en particiones al igual que **K-Means** que se conoce a veces como algoritmo **PAM (_Partitioning Around Medoids_)**. En _K-Medoids_, cada clúster está representado por un ___medoide___, que es el **punto de datos más cercano al centro del clúster**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08e7fc0",
      "metadata": {
        "id": "c08e7fc0"
      },
      "source": [
        "![350px-K-Medoids_Clustering.gif](attachment:350px-K-Medoids_Clustering.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70293a2",
      "metadata": {
        "id": "b70293a2"
      },
      "source": [
        "Una de las ventajas del algoritmo **K-Medoids** sobre **K-Means** es que es **menos sensible a los valores atípicos**, ya que **utiliza puntos de datos reales** como medoids, en lugar de la **media de los puntos de datos**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e4a541",
      "metadata": {
        "id": "11e4a541"
      },
      "source": [
        "Usamos el método **`make_blobs`** para generar un **bloque de datos gaussianos isotrópicos** pero eata vez con una **dispersión elevada** que nos permite comparar las dos técnicas de _K-Means_ y _K-Medoids_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca80576",
      "metadata": {
        "id": "6ca80576"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "datos_clust, etiquetas, centroides = make_blobs(n_samples=10000, centers=5, cluster_std=5, return_centers=True, random_state=10)\n",
        "\n",
        "# Datos generados sintéticamente\n",
        "print(centroides)\n",
        "print(etiquetas)\n",
        "print(datos_clust)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d566602",
      "metadata": {
        "id": "0d566602"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=datos_clust[:,0], y=datos_clust[:,1], hue=etiquetas)\n",
        "sns.scatterplot(x=centroides[:,0], y=centroides[:,1], color='green', s=90, label='Centroides')\n",
        "plt.title(\"Datos sintéticos con alta variabilidad\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d53294f",
      "metadata": {
        "id": "0d53294f"
      },
      "source": [
        "Procedemos a normalizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ae109a",
      "metadata": {
        "id": "85ae109a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado_clust = StandardScaler().fit(datos_clust)\n",
        "datos_clust_norm = escalado_clust.transform(datos_clust)\n",
        "datos_clust_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5cde0e",
      "metadata": {
        "id": "ab5cde0e"
      },
      "outputs": [],
      "source": [
        "# Verificar las características de los valores estandarizados\n",
        "display(pd.DataFrame(datos_clust).describe().round(2))\n",
        "display(pd.DataFrame(datos_clust_norm).describe().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b4ca21",
      "metadata": {
        "id": "33b4ca21"
      },
      "source": [
        "Creamos un modelo de _K-Means_ clustering, considerando el número de grupos a periori como `K=5`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316317ec",
      "metadata": {
        "id": "316317ec"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "modelo_kmeans = KMeans(n_clusters=5, random_state=100)\n",
        "\n",
        "modelo_kmeans.fit(datos_clust_norm)\n",
        "\n",
        "y_etiquetas = modelo_kmeans.labels_\n",
        "\n",
        "centros_clust = modelo_kmeans.cluster_centers_\n",
        "\n",
        "print(\"SSE = \", modelo_kmeans.inertia_)\n",
        "print(\"Silhouette score = \", silhouette_score(datos_clust_norm, y_etiquetas))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_etiquetas)\n",
        "sns.scatterplot(x=centros_clust[:,0], y=centros_clust[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con K-means\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a5deee",
      "metadata": {
        "id": "d4a5deee"
      },
      "outputs": [],
      "source": [
        "# Las etiquetas del modelo (clusters)\n",
        "y_etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016c843f",
      "metadata": {
        "id": "016c843f"
      },
      "outputs": [],
      "source": [
        "# El mapeo de etiquetas sabiendo los grupos creados sintéticamente\n",
        "y_km_label = np.where(y_etiquetas==0, 3, np.where(y_etiquetas==1, 2,\n",
        "                                         np.where(y_etiquetas==2, 4,\n",
        "                                         np.where(y_etiquetas==3, 1, 0))))\n",
        "y_km_label = pd.Series(y_km_label, name='label')\n",
        "y_km_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1ddb28",
      "metadata": {
        "id": "6f1ddb28"
      },
      "outputs": [],
      "source": [
        "# Visualizar datos reales vs. datos agrupados\n",
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=etiquetas, ax=axes[0])\n",
        "axes[0].set_title(\"Datos reales etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_km_label, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (K-Means)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4e54e8",
      "metadata": {
        "id": "9c4e54e8"
      },
      "outputs": [],
      "source": [
        "diff_pos = [i for i in range(len(y_km_label)) if y_km_label[i]!=etiquetas[i]]\n",
        "print(\"El modelo de clustering se ha euivocado en clasificar %s puntos\" % len(diff_pos))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos)/len(y_km_label))*100))\n",
        "y_km_label[diff_pos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9395cecb",
      "metadata": {
        "id": "9395cecb"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_km_label)\n",
        "sns.scatterplot(x=datos_clust_norm[:,0][diff_pos], y=datos_clust_norm[:,1][diff_pos], marker='o', color = 'red', s=30, label='errores')\n",
        "sns.scatterplot(x=centros_clust[:, 0], y=centros_clust[:, 1], color='blue', s=90, label='cluster_centers')\n",
        "plt.title(\"Resutados de clustering, los centros de cada cluster y los datos agrupados erroneamente\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27028e15",
      "metadata": {
        "id": "27028e15"
      },
      "source": [
        "Ahora vamos a instalar una librería que nos proporciona métodos adicionales a las de _sklearn_ y nos permite crear un modelo tipo _K-Medoids clustering_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d82d78f",
      "metadata": {
        "id": "6d82d78f"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn-extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e8947e",
      "metadata": {
        "id": "43e8947e"
      },
      "outputs": [],
      "source": [
        "# Comprobar la instalación del paquete\n",
        "import sklearn_extra.cluster\n",
        "\n",
        "print(\"sklearn_extra.cluster se ha importado correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5ee1ea",
      "metadata": {
        "id": "cf5ee1ea"
      },
      "source": [
        "Creamos al igual que el modelo _K-Means_ otro modelo basado en medoides considerando `K=5`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0842163f",
      "metadata": {
        "id": "0842163f"
      },
      "outputs": [],
      "source": [
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "modelo_kmedoids = KMedoids(n_clusters=5, random_state=100)\n",
        "\n",
        "modelo_kmedoids.fit(datos_clust_norm)\n",
        "\n",
        "y_etiquetas_md = modelo_kmedoids.labels_\n",
        "\n",
        "centros_clust_md = modelo_kmedoids.cluster_centers_\n",
        "\n",
        "print(\"SSE = \", modelo_kmedoids.inertia_)\n",
        "print(\"Silhouette score = \", silhouette_score(datos_clust_norm, y_etiquetas_md))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_etiquetas_md)\n",
        "sns.scatterplot(x=centros_clust[:, 0], y=centros_clust[:, 1], color='blue', s=60, label='centroids (K-Means)')\n",
        "sns.scatterplot(x=centros_clust_md[:,0], y=centros_clust_md[:,1], color='green', s=60, label='medoids (K-Medoids)')\n",
        "plt.title(\"Clustering con K-medoids\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390e94e2",
      "metadata": {
        "id": "390e94e2"
      },
      "source": [
        "Se puede observar que hay una diferencia entre algunos de los centros de clusters reconocidos por cada técnica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708be21e",
      "metadata": {
        "id": "708be21e"
      },
      "outputs": [],
      "source": [
        "y_etiquetas_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f46166e",
      "metadata": {
        "id": "0f46166e"
      },
      "outputs": [],
      "source": [
        "y_label_md = np.where(y_etiquetas_md==0, 2, np.where(y_etiquetas_md==1, 0,\n",
        "                                            np.where(y_etiquetas_md==2, 4,\n",
        "                                            np.where(y_etiquetas_md==3, 3, 1))))\n",
        "y_label_md = pd.Series(y_label_md, name='label')\n",
        "y_label_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf78238",
      "metadata": {
        "id": "fbf78238"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=etiquetas, ax=axes[0])\n",
        "axes[0].set_title(\"Datos reales etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_label_md, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (K-Medoids)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a742596",
      "metadata": {
        "id": "5a742596"
      },
      "outputs": [],
      "source": [
        "diff_pos_md = [i for i in range(len(y_label_md)) if y_label_md[i]!=etiquetas[i]]\n",
        "print(\"El modelo de clustering se ha euivocado en clasificar %s puntos\" % len(diff_pos_md))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_md)/len(y_label_md))*100))\n",
        "y_label_md[diff_pos_md]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4318cd1",
      "metadata": {
        "id": "a4318cd1"
      },
      "outputs": [],
      "source": [
        "print(\"El modelo K-Medoids ha equivicado en %s puntos menos que el modelo K-Means\" % (len(diff_pos)-len(diff_pos_md)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd120e4",
      "metadata": {
        "id": "6fd120e4"
      },
      "source": [
        "Vemos que el resultado de este modelo es algo **mejor**, a pesar de la **alta variación** de los datos que ha causado una **difusión** apreciable entre los clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879eeb4c",
      "metadata": {
        "id": "879eeb4c"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=datos_clust_norm[:,0], y=datos_clust_norm[:,1], hue=y_label_md)\n",
        "sns.scatterplot(x=datos_clust_norm[:,0][diff_pos], y=datos_clust_norm[:,1][diff_pos], marker='o', color = 'yellow', s=5, label='errores (K-Means)')\n",
        "sns.scatterplot(x=datos_clust_norm[:,0][diff_pos_md], y=datos_clust_norm[:,1][diff_pos_md], marker='o', color = 'red', s=5, label='errores (K-Medoids)')\n",
        "sns.scatterplot(x=centros_clust[:, 0], y=centros_clust[:, 1], color='blue', s=60, label='centeroids (K-Means)')\n",
        "sns.scatterplot(x=centros_clust_md[:,0], y=centros_clust_md[:,1], color='green', s=60, label='medoids (K-Medoids)')\n",
        "plt.title(\"Resutados de clustering, los centros de cada cluster y los datos agrupados erroneamente\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7da2b92",
      "metadata": {
        "id": "a7da2b92"
      },
      "source": [
        "Como se puede observar, el algoritmo _K-Means_ tiene unos **errores adicionales** al modelo _K-Medoids_ que se distribuyen principalmente en los **bordes de los clústeres** calculados con una **diferencia notable en la posición de sus centros**, muy probablamente **afectados por la dispersión elevada** en los puntos de datos.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389f564c",
      "metadata": {
        "id": "389f564c"
      },
      "source": [
        "### Métricas de evaluación del rendimiento de clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374c8e6a",
      "metadata": {
        "id": "374c8e6a"
      },
      "source": [
        "Evaluar el rendimiento de un algoritmo de clustering **no es tan sencillo** como contar el **número de errores** o la **precisión** de un algoritmo de clasificación supervisado.\n",
        "\n",
        "En concreto, **por no disponer de los verdaderos grupos de datos a periori** (si que haya alguno), tenemos que fijarnos para ver si el clustering:\n",
        "- En primer lugar **separa correctamente los datos no semejantes** y que\n",
        "- En segundo lugar los **miembros que pertenecen a la misma clase son más similares que otros** datos según alguna métrica de similitud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e5952f",
      "metadata": {
        "id": "44e5952f"
      },
      "source": [
        "#### 1- **Inertia** (WSS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c0f596",
      "metadata": {
        "id": "20c0f596"
      },
      "source": [
        "La distorsión o la inercia se conoce como la suma de las distancias al cuadrado (**Within-cluster Sum of Squares - WSS**) y se calcula midiendo la distancia entre cada punto de datos y su centroide, elevando al cuadrado esta distancia y sumando estos cuadrados en un grupo.  \n",
        "Una inercia **más baja** indica un **mejor** ajuste de los clusters a los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2618cb98",
      "metadata": {
        "id": "2618cb98"
      },
      "source": [
        "![inertia.png](attachment:inertia.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623b0588",
      "metadata": {
        "id": "623b0588"
      },
      "source": [
        "Es evidente que al tener un número mayor de clusters, la suma total de distancias cuadradas también baja. Pero la forma que evoluciona la **inercia** o la **distorsión** nos puede ayudar a la hora de elegir un valor razonable cómo el **núemro de los clusters (K)**. Esta técnica se conoce como el _método del codo (**[Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))**)_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa04cec",
      "metadata": {
        "id": "3aa04cec"
      },
      "source": [
        "![elbow.png](attachment:elbow.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697355e0",
      "metadata": {
        "id": "697355e0"
      },
      "source": [
        "Según esta gráfica tenemos que seleccionar el valor de ___K___ en el **codo**, es decir, el punto después del cual **la distorsión/inercia comienza a disminuir de forma lineal**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad3954a",
      "metadata": {
        "id": "7ad3954a"
      },
      "source": [
        "#### 2- **Silhouette score**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43c1801",
      "metadata": {
        "id": "b43c1801"
      },
      "source": [
        "El **criterio de la silueta o [Silhouette coefficient](https://en.wikipedia.org/wiki/Silhouette_(clustering))** mide de alguna manera la similitud que tiene una observación a su propio grupo en comparación con otros clusters. El **promedio** de los coeficientes para todos los puntos de datos se conoce como **Sillhouette score** que **cuanto mayor** sea ese puntaje, **mejor** calidad tendría el algoritmo de clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996dda10",
      "metadata": {
        "id": "996dda10"
      },
      "source": [
        "![silu1.png](attachment:silu1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b581c6c",
      "metadata": {
        "id": "8b581c6c"
      },
      "source": [
        "El coeficiente de silueta varía de **−1** a **+1**, donde un valor alto indica que **un punto está bien emparejado con su propio cluster y mal emparejado con los grupos vecinos**. Si la mayoría de los valores son altos, la configuración de clustering en es adecuada. Si muchos puntos tienen un valor bajo o negativo, es posible que haya demasiados o muy pocos clústeres."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68933788",
      "metadata": {
        "id": "68933788"
      },
      "source": [
        "#### 3- **Davies-Bouldin score** (DBI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f314a8a8",
      "metadata": {
        "id": "f314a8a8"
      },
      "source": [
        "La puntuación de Davies-Bouldin o **DBI (Davies–Bouldin index)** se define como la **similitud promedio entre clusters**, donde la similitud es una medida que **compara la distancia entre clusters con el tamaño** de estos mismos grupos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf0f16a",
      "metadata": {
        "id": "bdf0f16a"
      },
      "source": [
        "![DBI.png](attachment:DBI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3701bf81",
      "metadata": {
        "id": "3701bf81"
      },
      "source": [
        "Por lo tanto, los grupos que están **más separados y menos dispersos** darán como resultado una **puntuación menor** que undica una **mejor** calidad de los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5bb344",
      "metadata": {
        "id": "6c5bb344"
      },
      "source": [
        "#### 4- **Calinski-Harabasz index**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb911a3",
      "metadata": {
        "id": "edb911a3"
      },
      "source": [
        "El índice de **Calinski-Harabasz** conocido también como **Variance Ratio Criterion** mide la ratio de la varianza entre clusters y la varianza dentro de los clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bd9ccb",
      "metadata": {
        "id": "18bd9ccb"
      },
      "source": [
        "![calinski.jpg](attachment:calinski.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65058b6",
      "metadata": {
        "id": "f65058b6"
      },
      "source": [
        "Una **puntuación más alta** de _Calinski-Harabasz_ se relaciona con un modelo con grupos **mejor** definidos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f90273da",
      "metadata": {
        "id": "f90273da"
      },
      "source": [
        "#### 5- **Bayesian Information Criterion** (BIC)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21165a1",
      "metadata": {
        "id": "d21165a1"
      },
      "source": [
        "El criterio de información bayesiano (**BIC**) o el más general **criterio de Schwarz** es un criterio para la selección de modelos basado en la verosimilitud de los resultados considerando una penalización para el número de los parámetros. **Algunos artículos recientes han recomendado encarecidamente el uso de esta métrica en lugar de criterios convencionales como el de _Elbow method_**:  \n",
        " **[Stop using the elbow criterion for k-means and how to choose the number of clusters instead](https://arxiv.org/abs/2212.12189)**\n",
        "\n",
        "Más adelante probamos una implementación basado en el proyecto **[X-Means](https://github.com/bobhancock/goxmeans/tree/a78e909e374c6f97ddd04a239658c7c5b7365e5c)** traducido en [otro artículo](https://towardsdatascience.com/are-you-still-using-the-elbow-method-5d271b3063bd) desde el lenguaje _Go_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc64c1f",
      "metadata": {
        "id": "fbc64c1f"
      },
      "source": [
        "Vamos a crear un set de datos con un **número considerable de clústeres** y una **varianza mayor que la unidad**, con el fin de comprobar diferentes métricas de rendimiento de clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e634a16",
      "metadata": {
        "id": "4e634a16"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "datos_sint, etiquetas_sint, centroides_sint = make_blobs(n_samples=1000, centers=8, cluster_std=1.5, return_centers=True, random_state=10)\n",
        "sns.scatterplot(x=datos_sint[:,0], y=datos_sint[:,1])\n",
        "plt.title(\"Datos sintéticos sin etiquetas\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10f16ab",
      "metadata": {
        "id": "c10f16ab"
      },
      "source": [
        "Como se puede ver, el conjunto de datos contiene varios grupos que tampoco son fáciles de distinguir de forma visual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df32274f",
      "metadata": {
        "id": "df32274f"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=datos_sint[:,0], y=datos_sint[:,1], hue=etiquetas_sint)\n",
        "sns.scatterplot(x=centroides_sint[:,0], y=centroides_sint[:,1], color='green', s=90, label='Centroides')\n",
        "plt.title(\"Datos sintéticos etiquetados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d897f0",
      "metadata": {
        "id": "03d897f0"
      },
      "source": [
        "Estandarizamos los datos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c87eea3",
      "metadata": {
        "id": "4c87eea3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado_sint= StandardScaler().fit(datos_sint)\n",
        "datos_sint_norm = escalado_sint.transform(datos_sint)\n",
        "datos_sint_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519c1063",
      "metadata": {
        "id": "519c1063"
      },
      "outputs": [],
      "source": [
        "distor_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    distor_clust.append(modelo_km.inertia_)\n",
        "\n",
        "# Obtener una visualización más elaborada\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=distor_clust, color='green', label='WSS versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Suma de la distancia al cuadrado de cada punto a su centroide\", fontsize=16)\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"SSE\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ce07dc",
      "metadata": {
        "id": "c6ce07dc"
      },
      "outputs": [],
      "source": [
        "sil_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    sil_clust.append(silhouette_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=sil_clust, color='green', label='silueta versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Distancia media dentro del grupo (a) entre la distancia media del grupo más cercano (b)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"silhouette_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c51f24",
      "metadata": {
        "id": "19c51f24"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "dav_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    dav_clust.append(davies_bouldin_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=dav_clust, color='green', label='the Davies-Bouldin score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Davies-Bouldin index\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"davies_bouldin_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eb157e",
      "metadata": {
        "id": "40eb157e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "calinsk_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    calinsk_clust.append(calinski_harabasz_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=calinsk_clust, color='green', label='Calinski and Harabasz score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Variance Ratio Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"calinski_harabasz_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0568224b",
      "metadata": {
        "id": "0568224b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def bic_score(X, labels):\n",
        "  \"\"\"\n",
        "  BIC score for the goodness of fit of clusters.\n",
        "  \"\"\"\n",
        "\n",
        "  n_points = len(labels)\n",
        "  n_clusters = len(set(labels))\n",
        "  n_dimensions = X.shape[1]\n",
        "\n",
        "  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n",
        "\n",
        "  loglikelihood = 0\n",
        "  for label_name in set(labels):\n",
        "    X_cluster = X[labels == label_name]\n",
        "    n_points_cluster = len(X_cluster)\n",
        "    centroid = np.mean(X_cluster, axis=0)\n",
        "    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n",
        "    loglikelihood += \\\n",
        "      n_points_cluster * np.log(n_points_cluster) \\\n",
        "      - n_points_cluster * np.log(n_points) \\\n",
        "      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n",
        "      - (n_points_cluster - 1) / 2\n",
        "\n",
        "  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n",
        "\n",
        "  return bic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543702e3",
      "metadata": {
        "id": "543702e3"
      },
      "outputs": [],
      "source": [
        "bic_clust = []\n",
        "for k in range(2, 15):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_sint_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_sint_norm)\n",
        "    bic_clust.append(bic_score(datos_sint_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,15), y=bic_clust, color='green', label='BIC score versus K', linewidth=3)\n",
        "plt.xticks(range(2,15))\n",
        "plt.title(\"Bayesian Information Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9029c41f",
      "metadata": {
        "id": "9029c41f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045eb3bf",
      "metadata": {
        "id": "045eb3bf"
      },
      "source": [
        "###  Mean Shift Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a2d1422",
      "metadata": {
        "id": "9a2d1422"
      },
      "source": [
        "El algoritmo _Mean Shift_ es un algoritmo basado en centroide (_centroid-based_) funciona **desplazando los puntos de datos hacia los centroides en zonas más densas** para que sean la media de otros puntos de la región.\n",
        "\n",
        "Estos candidatos luego se filtran en una etapa de **posprocesamiento para eliminar casi duplicados** y de esta manera formar el conjunto final de centroides."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b00bcb",
      "metadata": {
        "id": "30b00bcb"
      },
      "source": [
        "![meanshift.gif](attachment:meanshift.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cae096",
      "metadata": {
        "id": "46cae096"
      },
      "source": [
        "La principal ventaja de _Mean Shift_ es un algoritmo **no paramétrico** que no requiere un conocimiento previo del núemro de grupos o el valor de _`K`_ en los datos. Para comprobarlo realizamos un ejercicio con los datos no distribuidos uniformamente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd5f80a",
      "metadata": {
        "id": "bbd5f80a"
      },
      "source": [
        "### Ejemplo de datos anisótropos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47bc281f",
      "metadata": {
        "id": "47bc281f"
      },
      "source": [
        "Los **datos distribuidos anisotrópicamente** se refieren a datos que **no están distribuidos uniformemente en todas las direcciones**, lo que significa que la **distribución de los datos varía según la dirección de observación**. Esto puede ocurrir cuando los datos exhiben algún tipo de dependencia direccional o estructural subyacente, como por ejemplo en **formaciones geológicas**.\n",
        "\n",
        "Por el contrario, los **datos isotrópicos se distribuyen uniformemente en todas las direcciones** y no muestran ninguna dependencia direccional. Los ejemplos de datos isotrópicos incluyen **muestras aleatorias de una distribución uniforme**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdf362d",
      "metadata": {
        "id": "4cdf362d"
      },
      "outputs": [],
      "source": [
        "data_aniso, etiquetas_aniso, centers_aniso = make_blobs(n_samples=1000, centers=([[-15,3],[0,6],[25,25]]), cluster_std=[1, 2, 3],\n",
        "                                                        return_centers=True, random_state=642)\n",
        "transform = [[1, -0.5], [-0.7, 0.8]]\n",
        "datos_aniso = np.dot(data_aniso, transform)\n",
        "centroides_aniso = np.dot(centers_aniso, transform)\n",
        "\n",
        "sns.scatterplot(x=datos_aniso[:,0], y=datos_aniso[:,1], hue=etiquetas_aniso)\n",
        "sns.scatterplot(x=centroides_aniso[:,0], y=centroides_aniso[:,1], color='green', s=90, label='Centroides')\n",
        "plt.title(\"Datos sintéticos distribuidos anisotrópicamente\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00b4e6c",
      "metadata": {
        "id": "d00b4e6c"
      },
      "outputs": [],
      "source": [
        "# Normalizar los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "escalado_aniso = StandardScaler().fit(datos_aniso)\n",
        "datos_aniso_norm = escalado_aniso.transform(datos_aniso)\n",
        "datos_aniso_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40a59ba",
      "metadata": {
        "id": "e40a59ba"
      },
      "outputs": [],
      "source": [
        "# Verificar las características de los valores estandarizados\n",
        "display(pd.DataFrame(datos_aniso).describe().round(2))\n",
        "display(pd.DataFrame(datos_aniso_norm).describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf7c929",
      "metadata": {
        "id": "3bf7c929"
      },
      "outputs": [],
      "source": [
        "distor_clust = []\n",
        "for k in range(2, 10):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_aniso_norm)\n",
        "    distor_clust.append(modelo_km.inertia_)\n",
        "\n",
        "# Obtener una visualización más elaborada\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,10), y=distor_clust, color='green', label='WSS versus K', linewidth=3)\n",
        "plt.xticks(range(2,10))\n",
        "plt.title(\"Suma de la distancia al cuadrado de cada punto a su centroide\", fontsize=16)\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"WSS\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97cb7d51",
      "metadata": {
        "id": "97cb7d51"
      },
      "outputs": [],
      "source": [
        "sil_clust = []\n",
        "for k in range(2, 10):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_aniso_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_aniso_norm)\n",
        "    sil_clust.append(silhouette_score(datos_aniso_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,10), y=sil_clust, color='green', label='silueta versus K', linewidth=3)\n",
        "plt.xticks(range(2,10))\n",
        "plt.title(\"Distancia media dentro del grupo (a) entre la distancia media del grupo más cercano (b)\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"silhouette_score\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bb210e",
      "metadata": {
        "id": "a8bb210e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "dav_clust = []\n",
        "for k in range(2, 10):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_aniso_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_aniso_norm)\n",
        "    dav_clust.append(davies_bouldin_score(datos_aniso_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,10), y=dav_clust, color='green', label='the Davies-Bouldin score versus K', linewidth=3)\n",
        "plt.xticks(range(2,10))\n",
        "plt.title(\"Davies-Bouldin index\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"davies_bouldin_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6b6ca3",
      "metadata": {
        "id": "7e6b6ca3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "calinsk_clust = []\n",
        "for k in range(2, 10):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_aniso_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_aniso_norm)\n",
        "    calinsk_clust.append(calinski_harabasz_score(datos_aniso_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,10), y=calinsk_clust, color='green', label='Calinski and Harabasz score versus K', linewidth=3)\n",
        "plt.xticks(range(2,10))\n",
        "plt.title(\"Variance Ratio Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"calinski_harabasz_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8fd5a0",
      "metadata": {
        "id": "bb8fd5a0"
      },
      "outputs": [],
      "source": [
        "bic_clust = []\n",
        "for k in range(2, 10):\n",
        "    modelo_km = KMeans(n_clusters=k, random_state=100)\n",
        "    modelo_km.fit(datos_aniso_norm)\n",
        "    y_modelo_km = modelo_km.predict(datos_aniso_norm)\n",
        "    bic_clust.append(bic_score(datos_aniso_norm, y_modelo_km))\n",
        "\n",
        "# Obtener una visualización del valor de silueta\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.lineplot(x=range(2,10), y=bic_clust, color='green', label='BIC score versus K', linewidth=3)\n",
        "plt.xticks(range(2,10))\n",
        "plt.title(\"Bayesian Information Criterion\")\n",
        "plt.xlabel(\"Número de Clusters\", fontsize=14)\n",
        "plt.ylabel(\"bic_score\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024130ea",
      "metadata": {
        "id": "024130ea"
      },
      "source": [
        "Se puede apreciar que ninguna métrica ha podido estimar correctamente el número de los grupos presentes en los datos. Procedemos a crear un modelo de clustering usando la técnica de ***Mean Shift*** que no requiere conocer el hiperparametro de `K` a periori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201280b2",
      "metadata": {
        "id": "201280b2"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "modelo_ms = MeanShift(bin_seeding=True)\n",
        "\n",
        "modelo_ms.fit(datos_aniso_norm)\n",
        "\n",
        "y_etiquetas_ms = modelo_ms.labels_\n",
        "\n",
        "centros_aniso_ms = modelo_ms.cluster_centers_\n",
        "\n",
        "print(\"Silhouette score = \", silhouette_score(datos_aniso_norm, y_etiquetas_ms))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=y_etiquetas_ms)\n",
        "sns.scatterplot(x=centros_aniso_ms[:,0], y=centros_aniso_ms[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con Mean Shift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab041890",
      "metadata": {
        "id": "ab041890"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=etiquetas_aniso, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos con etiquetas\")\n",
        "\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=y_etiquetas_ms, ax=axes[1])\n",
        "axes[1].set_title(\"Datos etiquetados por clustering (Mean Shift)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf98b0e4",
      "metadata": {
        "id": "cf98b0e4"
      },
      "outputs": [],
      "source": [
        "diff_pos_ms = [i for i in range(len(y_etiquetas_ms)) if y_etiquetas_ms[i]!=etiquetas_aniso[i]]\n",
        "print(\"El modelo de clustering se ha euivocado en clasificar %s puntos\" % len(diff_pos_ms))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_ms)/len(y_etiquetas_ms))*100))\n",
        "y_etiquetas_ms[diff_pos_ms]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "363abed8",
      "metadata": {
        "id": "363abed8"
      },
      "source": [
        "Ahora desarrollamos un algoritmo de _K-Means clustering_ suponiendo conocer previamente el número de los grupos existentes en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00881059",
      "metadata": {
        "id": "00881059"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "modelo_km_aniso = KMeans(n_clusters=3, random_state=100)\n",
        "\n",
        "modelo_km_aniso.fit(datos_aniso_norm)\n",
        "\n",
        "y_km_aniso = modelo_km_aniso.labels_\n",
        "\n",
        "centros_aniso_km = modelo_km_aniso.cluster_centers_\n",
        "\n",
        "print(\"SSE = \", modelo_km_aniso.inertia_)\n",
        "print(\"Silhouette score = \", silhouette_score(datos_aniso_norm, y_km_aniso))\n",
        "\n",
        "# Graficar los resultados\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=y_km_aniso)\n",
        "sns.scatterplot(x=centros_aniso_km[:,0], y=centros_aniso_km[:,1], color='blue', s=60, label='cluster_centers')\n",
        "plt.title(\"Clustering con K-means\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20cd3bc5",
      "metadata": {
        "id": "20cd3bc5"
      },
      "outputs": [],
      "source": [
        "y_label_km_aniso = np.where(y_km_aniso==0, 2, np.where(y_km_aniso==1, 0, 1))\n",
        "y_label_km_aniso = pd.Series(y_label_km_aniso, name='label')\n",
        "y_label_km_aniso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e802f9",
      "metadata": {
        "id": "f2e802f9"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, figsize=(20,7))\n",
        "\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=etiquetas_aniso, ax=axes[0])\n",
        "axes[0].set_title(\"Datos sintéticos etiquetados\")\n",
        "\n",
        "sns.scatterplot(x=datos_aniso_norm[:,0], y=datos_aniso_norm[:,1], hue=y_label_km_aniso, ax=axes[1])\n",
        "axes[1].set_title(\"Datos agrupados por clustering (K-Means)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa572336",
      "metadata": {
        "id": "fa572336"
      },
      "outputs": [],
      "source": [
        "diff_pos_km_aniso = [i for i in range(len(y_label_km_aniso)) if y_label_km_aniso[i]!=etiquetas_aniso[i]]\n",
        "print(\"El modelo de clustering se ha euivocado en clasificar %s puntos\" % len(diff_pos_km_aniso))\n",
        "print(\"El porcentage de acierto es {acc}%\".format(acc=(1-len(diff_pos_km_aniso)/len(y_label_km_aniso))*100))\n",
        "y_label_km_aniso[diff_pos_ms]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae717708",
      "metadata": {
        "id": "ae717708"
      },
      "source": [
        "El modelo de _Mean Shift_ ha sacado mejore resultados (_aproximadamente 10 %_) y encima sin necesidad de saber de antemano el número de los clústeres."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694af418",
      "metadata": {
        "id": "694af418"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9480e05",
      "metadata": {
        "id": "f9480e05"
      },
      "source": [
        "### **`Ejercicio 21.1`**\n",
        "\n",
        "Vamos a revisar algunas fuentes de datos en internet para usar el concepto de **Open Data** o datasets disponibles al público que nos permiten practicar diferentes problemas de tipo **Regresión**, **Clasificación** o **Clustring** aplicando varias técnicas **que hemos aprendido hasta ahora y en las sesiones anteriores**.\n",
        "\n",
        "De este modo cada alumn@ escogería un **proyecto distinto** que **no se haya utilizado a lo largo de este curso** para desarrollarlo y presentarlo como el **`proyecto final del módulo1`**.\n",
        "\n",
        "- **[Top 23 Best Public Datasets for Practicing Machine Learning](https://rubikscode.net/2021/07/19/top-23-best-public-datasets-for-practicing-machine-learning/)**: Una colección de casos de uso de [Rubik’s Code](https://rubikscode.net/) escogidos por [Nikola Živković](https://rubikscode.net/deep-learning-for-programmers/#:~:text=ABOUT%20THE%20AUTHOR-,Nikola%20%C5%BDivkovi%C4%87,-is%20a%20software).  \n",
        "\n",
        "- **[The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php)**: Uno de los repositorios más comúnes entre la comunidad global de científicos de datos con cientos de ejemplos basados en datos reales.\n",
        "\n",
        "**`21.1`** Elige un caso de uso según los ejemplos que se encuentran en las **dos fuentes indicadas** y entrega el conjunto de datos a utilizar para el proyecto como un **adjunto a la tarea**.\n",
        "\n",
        "**`21.2`** Prepara una **presentación** resumiendo en qué consiste el **dataset**, el **planteamiento del problema** y tu **propuesta** para resolver el problema sin llegar a desarrollarlo todavía.\n",
        "\n",
        "...and that's it all!!!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}