{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "496631ea",
      "metadata": {
        "id": "496631ea"
      },
      "source": [
        "# Módulo 1: Análisis de datos en el ecosistema Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a14cd3f",
      "metadata": {
        "id": "3a14cd3f"
      },
      "source": [
        "### Sesión (26)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e06d26c",
      "metadata": {
        "id": "5e06d26c"
      },
      "source": [
        "# Machine Learning Time Series Forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245ee769",
      "metadata": {
        "id": "245ee769"
      },
      "source": [
        "Para aplicar modelos de aprendizaje automático (___Machine Learning___) a problemas de predicción de series temporales, **debemos transformar la serie en una matriz** en la que cada valor esté asociado a la ventana temporal con las observaciones anteriores (lags) correspondientes. Para ello tenemos que crear conjuntos de datos con las observaciones pasadas acompañadas con el último valor de la serie después de la ventana temporal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191b2651",
      "metadata": {
        "id": "191b2651"
      },
      "source": [
        "![transform_timeseries.gif](attachment:transform_timeseries.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047b25dd",
      "metadata": {
        "id": "047b25dd"
      },
      "source": [
        "De este modo, conseguimos **convertir el problema de forecasting** o la predicción de los valores de una serie temporal **a un problema tipo regresión**, donde el modelo entrenado es capaz de calcular el próximo valor de una serie **teniendo las secuencias anteriores según el tamaño de la ventana temporal elegida** en la fase preparación de datos.  \n",
        "\n",
        "Una vez que los datos se han reorganizado en la nueva forma, se puede **entrenar cualquier modelo de regresión** para predecir el próximo valor de la serie. Durante el entrenamiento del modelo, cada fila se considera una instancia o conjunto de datos, donde los valores en los retrasos $1, 2, ... p$ se consideran como **variables independientes o predictores** de la cantidad de la serie temporal en el paso de tiempo $p+1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf5164f",
      "metadata": {
        "id": "dcf5164f"
      },
      "source": [
        "![diagram-trainig-forecaster.png](attachment:diagram-trainig-forecaster.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e2a03b",
      "metadata": {
        "id": "b0e2a03b"
      },
      "source": [
        "### Recursive multi-step forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea56b13",
      "metadata": {
        "id": "8ea56b13"
      },
      "source": [
        "Dado que se requiere el valor $t_{(n-1)}$ para predecir $t_{(n)}$, cuando se desconoce $t_{(n-1)}$, podemos **aplicar un proceso recursivo** en el que, cada nuavo valor calculado, se basa en la anterior. Este proceso se conoce como predicción recursiva (___recursive forecasting___)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9c7a14",
      "metadata": {
        "id": "1f9c7a14"
      },
      "source": [
        "![diagram-recursive-mutistep-forecasting.png](attachment:diagram-recursive-mutistep-forecasting.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e211f2be",
      "metadata": {
        "id": "e211f2be"
      },
      "source": [
        "### Direct multi-step forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f68618",
      "metadata": {
        "id": "49f68618"
      },
      "source": [
        "La **predicción directa** de varios pasos consiste en **entrenar un modelo diferente para cada paso del horizonte** de predicción. Como resultado, las predicciones son independientes entre sí."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840d28a0",
      "metadata": {
        "id": "840d28a0"
      },
      "source": [
        "![diagram-direct-multi-step-forecasting.png](attachment:diagram-direct-multi-step-forecasting.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bb3b16",
      "metadata": {
        "id": "12bb3b16"
      },
      "outputs": [],
      "source": [
        "# importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac9270b",
      "metadata": {
        "id": "2ac9270b"
      },
      "outputs": [],
      "source": [
        "# Modificamos los parámetros de los gráficos en matplotlib\n",
        "from matplotlib.pyplot import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 6 # el primer dígito es el ancho y el segundo el alto\n",
        "rcParams[\"font.weight\"] = \"bold\"\n",
        "rcParams[\"font.size\"] = 10\n",
        "rcParams[\"axes.labelweight\"] = \"bold\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddc1ae8",
      "metadata": {
        "id": "5ddc1ae8"
      },
      "source": [
        "### Airline Passenger Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88855ca1",
      "metadata": {
        "id": "88855ca1"
      },
      "source": [
        "Importamos los datos del ejemplo disponible en la librería ___seaborn___ que contiene el número total de pasajeros aéreos de forma mensual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04bbbca",
      "metadata": {
        "id": "e04bbbca"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el dataset de \"flights\"\n",
        "df_air = sns.load_dataset('flights')\n",
        "df_air"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f265acb",
      "metadata": {
        "id": "0f265acb"
      },
      "outputs": [],
      "source": [
        "# Consultamos la información del dataset descargado\n",
        "df_air.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb393bc",
      "metadata": {
        "id": "5cb393bc"
      },
      "outputs": [],
      "source": [
        "# Crear una nueva columna con la unificación de otras dos\n",
        "df_air['year_month'] = df_air.apply(lambda x: str(x['year']) + '-' + x['month'], axis=1)\n",
        "\n",
        "# Convertir la columna en fechas tipo DatetimeIndex\n",
        "df_air['fechas'] = pd.to_datetime(df_air['year_month'], format='%Y-%b')\n",
        "\n",
        "# Convertir la columna de fechas a los índices del DataFrame e indicar que los datos son \"mensuales\"\n",
        "df_air.set_index('fechas', inplace=True)\n",
        "df_air.index.freq = 'MS'\n",
        "\n",
        "# Quitar las columnas no necesarias\n",
        "df_air.drop(columns=['year', 'month', 'year_month'], inplace=True)\n",
        "\n",
        "df_air"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8381d2b",
      "metadata": {
        "id": "a8381d2b"
      },
      "outputs": [],
      "source": [
        "# Visualizar el DataFrame creado con los datos de la serie temporal\n",
        "plt.plot(df_air)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43733d8c",
      "metadata": {
        "id": "43733d8c"
      },
      "outputs": [],
      "source": [
        "# Definir el periodo de prueba (horizonte de predicción)\n",
        "horizonte = 12  # La cantidad de puntos a predecir\n",
        "df_test = df_air.tail(horizonte)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933d5fac",
      "metadata": {
        "id": "933d5fac"
      },
      "outputs": [],
      "source": [
        "# Filtrar la serie original para sacar el periodo de entrenamiento\n",
        "df_train = df_air[df_air.index.isin(df_test.index)==False]\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3c35f8",
      "metadata": {
        "id": "3b3c35f8"
      },
      "source": [
        "### Recursive autoregressive forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04675a2",
      "metadata": {
        "id": "e04675a2"
      },
      "source": [
        "__`Skforecast`__ es una libraría de _Python_ para la **previsión de series temporales basada en _scikit-learn_**. Este paquete proporciona un conjunto de herramientas y modelos para trabajar con datos de series temporales, incluido el preprocesamiento, la ingeniería de variables y la predicción automatizada.\n",
        "\n",
        "La principal ventaja de _skforecast_ es que permite el uso de la conocida interfaz _scikit-learn_ para **convertir los problemas de previsión de series temporales en problemas de regresión** y de este modo poder **aplicar distintas técnicas de Machine Learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4ab242",
      "metadata": {
        "id": "7a4ab242"
      },
      "outputs": [],
      "source": [
        "pip install skforecast"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb6345a",
      "metadata": {
        "id": "4cb6345a"
      },
      "source": [
        "Creamos un modelo autoregresivo usando bosques aleatrorios (_random forest_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32a2007",
      "metadata": {
        "id": "c32a2007"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_rf = ForecasterAutoreg(\n",
        "                regressor = RandomForestRegressor(random_state=77),\n",
        "                lags = 12\n",
        "                )\n",
        "\n",
        "mod_rf.fit(y=df_train['passengers'])\n",
        "mod_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20765520",
      "metadata": {
        "id": "20765520"
      },
      "outputs": [],
      "source": [
        "pred_rf = mod_rf.predict(steps=horizonte)\n",
        "pred_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15966c1b",
      "metadata": {
        "id": "15966c1b"
      },
      "outputs": [],
      "source": [
        "pred_rf = pred_rf.round()\n",
        "pred_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ead5e2",
      "metadata": {
        "id": "a2ead5e2"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_rf, label='Predicción - RF')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c67ca9c",
      "metadata": {
        "id": "8c67ca9c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_rf))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_rf)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_rf))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_rf)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3196135c",
      "metadata": {
        "id": "3196135c"
      },
      "source": [
        "##### **In-Sample** Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad724ff3",
      "metadata": {
        "id": "ad724ff3"
      },
      "outputs": [],
      "source": [
        "# Crear los vectores de entrenamiento\n",
        "X_train, y_train = mod_rf.create_train_X_y(df_train['passengers'])\n",
        "\n",
        "# Calcular los valores del modelo en el periodo de entrenamiento\n",
        "fitted_values = mod_rf.regressor.predict(X_train)\n",
        "fitted_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f126235",
      "metadata": {
        "id": "1f126235"
      },
      "outputs": [],
      "source": [
        "# Ordenar las predicciones del modelo para el periodo de emtrenamiento\n",
        "estim_rf = pd.Series(data=np.zeros(df_train.size), index=df_train.index, name='fitted')\n",
        "estim_rf[:horizonte] = np.nan\n",
        "estim_rf[horizonte:] = fitted_values\n",
        "estim_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c92cb2b",
      "metadata": {
        "id": "0c92cb2b"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_train, label='Entrenamiento')\n",
        "plt.plot(estim_rf, label='Estimación RandomForestRegressor')\n",
        "plt.title(\"Datos reales vs. Estimación del modelo (In-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fb1c84",
      "metadata": {
        "id": "f0fb1c84"
      },
      "outputs": [],
      "source": [
        "# Comparar los valores reales con la estimación del modelo\n",
        "sns.scatterplot(x=df_train['passengers'], y=estim_rf)\n",
        "plt.plot(estim_rf, estim_rf, color='r', linestyle=':')\n",
        "plt.title(\"Valores reales vs. valors estimados (In-Sample forecasting) RandomForestRegressor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74de8de1",
      "metadata": {
        "id": "74de8de1"
      },
      "outputs": [],
      "source": [
        "# Calcular los valores del componente residual (In-sample errors)\n",
        "resid_rf = df_train['passengers']-estim_rf\n",
        "resid_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08506a51",
      "metadata": {
        "id": "08506a51"
      },
      "outputs": [],
      "source": [
        "# Las estadísticas del componente residual\n",
        "resid_rf.describe().round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb70916d",
      "metadata": {
        "id": "eb70916d"
      },
      "outputs": [],
      "source": [
        "plt.plot(resid_rf)\n",
        "plt.title(\"Componente residual del modelo RandomForestRegressor (In-Sample fitted errores)\")\n",
        "plt.axhline(y=0, color='r', linestyle=':')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbaa051",
      "metadata": {
        "id": "3bbaa051"
      },
      "outputs": [],
      "source": [
        "# El histograma del componente residual (In-sample errors)\n",
        "sns.histplot(data=resid_rf, bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8860a1b6",
      "metadata": {
        "id": "8860a1b6"
      },
      "outputs": [],
      "source": [
        "win = 20\n",
        "resid_rf_std = resid_rf.rolling(win).std().iloc[win-1::win]\n",
        "plt.plot(resid_rf_std, label='Desviación estándar')\n",
        "plt.axhline(y=resid_rf.std(), color='r', linestyle='--')\n",
        "plt.title(\"Características estadísticas: Residual - RandomForestRegressor\")\n",
        "plt.ylim(0,50)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6b42b5",
      "metadata": {
        "id": "7f6b42b5"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 7\n",
        "plot_acf(resid_rf.dropna(), lags=37)\n",
        "plt.xticks(np.arange(37))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3c5fad",
      "metadata": {
        "id": "7f3c5fad"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "plot_pacf(resid_rf.dropna(), lags=37, method='ywm')\n",
        "plt.xticks(np.arange(37))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b41f5c",
      "metadata": {
        "id": "d8b41f5c"
      },
      "source": [
        "Vemos que hay algo de correlación presente en los valores del componente residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288fb49f",
      "metadata": {
        "id": "288fb49f"
      },
      "outputs": [],
      "source": [
        "# Analizamos el componente residual\n",
        "\n",
        "# Coeficiente de correlación entre valores reales y los errores\n",
        "print(df_train['passengers'].corr(resid_rf).round(4))\n",
        "\n",
        "# Coeficiente de correlación entre valores estimados y los errores\n",
        "print(estim_rf.corr(resid_rf).round(4))\n",
        "\n",
        "sns.scatterplot(x=df_train['passengers'], y=resid_rf)\n",
        "plt.title(\"Valores reales versus valores residuales - RandomForestRegressor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a19d6ec",
      "metadata": {
        "id": "4a19d6ec"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=estim_rf, y=resid_rf)\n",
        "plt.title(\"Valores estimados versus valores residuales - RandomForestRegressor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61263f14",
      "metadata": {
        "id": "61263f14"
      },
      "source": [
        "#### Analizar el intervalo de confianza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614c24ba",
      "metadata": {
        "id": "614c24ba"
      },
      "outputs": [],
      "source": [
        "conf_rf = mod_rf.predict_interval(steps=horizonte, interval=[0,100], random_state=111)\n",
        "conf_rf.index = df_test.index\n",
        "conf_rf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72c2edb",
      "metadata": {
        "id": "d72c2edb"
      },
      "source": [
        "Sacamos la banda que se aplica como intervalo de confianza: _delta_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7849c2f8",
      "metadata": {
        "id": "7849c2f8"
      },
      "outputs": [],
      "source": [
        "conf_rf['delta'] = conf_rf.apply(lambda x: x['upper_bound'] - x['lower_bound'], axis=1)\n",
        "conf_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d437e1a5",
      "metadata": {
        "id": "d437e1a5"
      },
      "outputs": [],
      "source": [
        "plt.plot(conf_rf['delta'])\n",
        "plt.title(\"Evolución del rango de los intervalos de confianza  - RandomForestRegressor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d8b495",
      "metadata": {
        "id": "44d8b495"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_train.index,\n",
        "    y=df_train['passengers'],\n",
        "    name=\"Entrenamiento\",\n",
        "    mode=\"lines\"\n",
        "    ))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_test.index,\n",
        "    y=df_test['passengers'],\n",
        "    name=\"Test\",\n",
        "    mode=\"lines\"\n",
        "    ))\n",
        "\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pred_rf.index,\n",
        "    y=pred_rf,\n",
        "    name=\"Predicción (RandomForestRegressor - 12)\",\n",
        "    mode=\"markers+lines\"\n",
        "    ))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=conf_rf.index,\n",
        "    y=conf_rf['lower_bound'],\n",
        "    name=\"lower\",\n",
        "    mode=\"lines\",\n",
        "    line=dict(width=0),\n",
        "    showlegend=False\n",
        "    ))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=conf_rf.index,\n",
        "    y=conf_rf['upper_bound'],\n",
        "    name=\"upper\",\n",
        "    mode=\"lines\",\n",
        "    line=dict(width=0),\n",
        "    showlegend=False,\n",
        "    fillcolor='rgba(68, 68, 68, 0.3)',\n",
        "    fill='tonexty'\n",
        "    ))\n",
        "\n",
        "fig.update_layout(title=\"Número de pasajeros aéreos de cada mes desde el año 1949 al 1960\",\n",
        "                  title_font_size=22,\n",
        "                  xaxis_title = 'Fecha',\n",
        "                  yaxis_title= 'Pasajeros'\n",
        "                  )\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4cf0bc",
      "metadata": {
        "id": "1d4cf0bc"
      },
      "source": [
        "### Ajustar los hiperparámetros\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c97256",
      "metadata": {
        "id": "d3c97256"
      },
      "source": [
        "Una técnica común para encontrar la ventana de tiempo que marca los retrasos necesarios para modelar la serie temporal consiste en analizar el _correlograma_ normal y _parcial_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8efa57",
      "metadata": {
        "id": "ae8efa57"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 7\n",
        "plot_acf(df_train, lags=20)\n",
        "plt.xticks(np.arange(20))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4987739",
      "metadata": {
        "id": "f4987739"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "plot_pacf(df_train, lags=50, method='ywm')\n",
        "plt.xticks(np.arange(50))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dac994b",
      "metadata": {
        "id": "5dac994b"
      },
      "source": [
        "Podemos considerar una serie de posibles retrasos o _lags_ que sean importantes de cara al modelo:\n",
        "- $1, 2, 3, ...., 25$\n",
        "- $1, 2, 9, 12, 13$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f37595",
      "metadata": {
        "id": "88f37595"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(1,26).tolist()\n",
        "hiper_param.append([1,2,9,12,13])\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(len(hiper_param))\n",
        "\n",
        "for i in range(len(hiper_param)):\n",
        "    # Generamos un modelo para cada hiperparámetro, lo entrenamos y calculamos el R_cuadrado sobre datos de test\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = RandomForestRegressor(random_state=77),\n",
        "                    lags = hiper_param[i]\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_train['passengers'])\n",
        "    test_r2[i] = r2_score(df_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de lags podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado de test\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(list(map(str, hiper_param)), test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(str(hiper_param[np.argmax(test_r2)]), max(test_r2),\n",
        "         marker='o', color = \"red\", label=\"max R^2\")\n",
        "\n",
        "plt.xlabel('Complejidad (lags)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5029458b",
      "metadata": {
        "id": "5029458b"
      },
      "source": [
        "La librería _skforecast_ cuanta con un método propio llamado `grid_search_forecaster` que es una implementación de _GridSearch_ para encontrar la combinación óptima de los hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587825cb",
      "metadata": {
        "id": "587825cb"
      },
      "outputs": [],
      "source": [
        "from skforecast.model_selection import grid_search_forecaster\n",
        "\n",
        "# Modelo\n",
        "forecaster = ForecasterAutoreg(\n",
        "                regressor = RandomForestRegressor(random_state=77),\n",
        "                lags      = 12\n",
        "             )\n",
        "\n",
        "# Lags\n",
        "lags_grid = [12, 13, [1,2,9,12,13]]\n",
        "\n",
        "# Parámetros del regresor\n",
        "param_grid = {'n_estimators': [100, 300, 500],\n",
        "              'max_depth': [5, 10, 20, 30]}\n",
        "\n",
        "results_grid = grid_search_forecaster(\n",
        "                        forecaster         = forecaster,\n",
        "                        y                  = df_train['passengers'],\n",
        "                        param_grid         = param_grid,\n",
        "                        lags_grid          = lags_grid,\n",
        "                        steps              = horizonte,\n",
        "                        refit              = False,\n",
        "                        metric             = 'mean_absolute_error',\n",
        "                        initial_train_size = int(len(df_train)*0.5),\n",
        "                        fixed_train_size   = False,\n",
        "                        return_best        = True,\n",
        "                        verbose            = False\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48492af2",
      "metadata": {
        "id": "48492af2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_rf_grid = ForecasterAutoreg(\n",
        "                regressor = RandomForestRegressor(n_estimators=500,\n",
        "                                                  max_depth=5,\n",
        "                                                  random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_rf_grid.fit(y=df_train['passengers'])\n",
        "pred_rf_grid = mod_rf_grid.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_rf_grid))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_rf_grid)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_rf_grid))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_rf_grid)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_rf_grid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c556022e",
      "metadata": {
        "id": "c556022e"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_rf_grid, label='Predicción - RF (grid)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f9ebf1",
      "metadata": {
        "id": "93f9ebf1"
      },
      "source": [
        "Además de este método podemos usar el análisis de complejidad como problemas de regresión para ver la evolución al cambiar cada hiperparámetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7b4619",
      "metadata": {
        "id": "1f7b4619"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(5,501,5)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = RandomForestRegressor(n_estimators=hiper_param[i],\n",
        "                                                      random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_train['passengers'])\n",
        "    test_r2[i] = r2_score(df_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de n_estimator podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (n_estimator)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc3dcfd",
      "metadata": {
        "id": "bbc3dcfd"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(2,30)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = RandomForestRegressor(max_depth=hiper_param[i],\n",
        "                                                      n_estimators=10,\n",
        "                                                      random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_train['passengers'])\n",
        "    test_r2[i] = r2_score(df_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de max_depth podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado de training versus de test\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (max_depth)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a6daab",
      "metadata": {
        "id": "74a6daab"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_rf_opt = ForecasterAutoreg(\n",
        "                regressor = RandomForestRegressor(n_estimators=10,\n",
        "                                                  max_depth=10,\n",
        "                                                  random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_rf_opt.fit(y=df_train['passengers'])\n",
        "pred_rf_opt = mod_rf_opt.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_rf_opt))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_rf_opt)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_rf_opt))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_rf_opt)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_rf_opt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691f972e",
      "metadata": {
        "id": "691f972e"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_rf_opt, label='Predicción - RF (óptimo)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cac9fee",
      "metadata": {
        "id": "7cac9fee"
      },
      "source": [
        "Gracias a la conversión realizada por _skforecast_ podemos aplicar distintas técnicas de ML que se pueden aplicar en los problemas de regresión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21daf0e",
      "metadata": {
        "id": "d21daf0e"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557be3de",
      "metadata": {
        "id": "557be3de"
      },
      "source": [
        "Esta vez usamos los modelos de _XGBoost_ para modelar la serie temporal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9b6968",
      "metadata": {
        "id": "2b9b6968"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_xgb = ForecasterAutoreg(\n",
        "                regressor = XGBRegressor(random_state=77),\n",
        "                lags = 12\n",
        "                )\n",
        "\n",
        "mod_xgb.fit(y=df_train['passengers'])\n",
        "pred_xgb = mod_xgb.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_xgb))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_xgb)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_xgb))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_xgb)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_xgb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6e15cf",
      "metadata": {
        "id": "0b6e15cf"
      },
      "outputs": [],
      "source": [
        "from skforecast.model_selection import grid_search_forecaster\n",
        "\n",
        "forecaster = ForecasterAutoreg(\n",
        "                regressor = XGBRegressor(random_state=77),\n",
        "                lags      = 12\n",
        "             )\n",
        "\n",
        "# Lags\n",
        "lags_grid = [12, 13, [1,2,9,12,13]]\n",
        "\n",
        "# Parámetros del regresor\n",
        "param_grid = {'n_estimators': [100, 300, 500],\n",
        "              'max_depth': [5, 10, 20, 30],\n",
        "              'learning_rate': [0.01, 0.1, 0.5, 1]}\n",
        "\n",
        "results_grid_xgb = grid_search_forecaster(\n",
        "                        forecaster         = forecaster,\n",
        "                        y                  = df_train['passengers'],\n",
        "                        param_grid         = param_grid,\n",
        "                        lags_grid          = lags_grid,\n",
        "                        steps              = horizonte,\n",
        "                        refit              = False,\n",
        "                        metric             = 'mean_absolute_error',\n",
        "                        initial_train_size = int(len(df_train)*0.5),\n",
        "                        fixed_train_size   = False,\n",
        "                        return_best        = True,\n",
        "                        verbose            = False\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc1e44bc",
      "metadata": {
        "id": "dc1e44bc"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_xgb_grid = ForecasterAutoreg(\n",
        "                regressor = XGBRegressor(n_estimators=300,\n",
        "                                         max_depth=10,\n",
        "                                         learning_rate=0.1,\n",
        "                                         random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_xgb_grid.fit(y=df_train['passengers'])\n",
        "pred_xgb_grid = mod_xgb_grid.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_xgb_grid))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_xgb_grid)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_xgb_grid))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_xgb_grid)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_xgb_grid))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91fbdfd7",
      "metadata": {
        "id": "91fbdfd7"
      },
      "source": [
        "Ahora volvemos a analizar los hiperparámetros de uno en uno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd3636a",
      "metadata": {
        "id": "4fd3636a"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(5,501,5)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = XGBRegressor(n_estimators=hiper_param[i],\n",
        "                                                      random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_train['passengers'])\n",
        "    test_r2[i] = r2_score(df_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de n_estimator podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado de training versus de test\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (n_estimator)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a432b9da",
      "metadata": {
        "id": "a432b9da"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(2,30)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = XGBRegressor(max_depth=hiper_param[i],\n",
        "                                                      n_estimators=50,\n",
        "                                                      random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_train['passengers'])\n",
        "    test_r2[i] = r2_score(df_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de max_depth podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (max_depth)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0780ddb5",
      "metadata": {
        "id": "0780ddb5"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_xgb_grid = ForecasterAutoreg(\n",
        "                regressor = XGBRegressor(n_estimators=50,\n",
        "                                         max_depth=10,\n",
        "                                         random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_xgb_grid.fit(y=df_train['passengers'])\n",
        "pred_xgb_grid = mod_xgb_grid.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_xgb_grid))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_xgb_grid)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_xgb_grid))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_xgb_grid)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_xgb_grid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "045999dc",
      "metadata": {
        "id": "045999dc"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_xgb_grid, label='Predicción - XGBRegressor (opt)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58100d70",
      "metadata": {
        "id": "58100d70"
      },
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4321c05e",
      "metadata": {
        "id": "4321c05e"
      },
      "source": [
        "LightGBM (`Light Gradient Boosting Machine`) es una herramienta de alto rendimiento desarrollado por _Microsoft_ que utiliza algoritmos de árboles de decisión para tareas de aprendizaje supervisado como regresión y clasificación.  \n",
        "\n",
        "_LightGBM_ está escrito en __C++__, pero también tiene una interfaz de _Python_ y se puede usar junto con librerías populares de análisis de datos y aprendizaje automático, como pandas, _scikit-learn_ y _XGBoost_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a5339e",
      "metadata": {
        "id": "74a5339e"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "mod_lgbm = ForecasterAutoreg(\n",
        "                regressor = LGBMRegressor(random_state=77),\n",
        "                lags = 12\n",
        "                )\n",
        "\n",
        "mod_lgbm.fit(y=df_train['passengers'])\n",
        "pred_lgbm = mod_lgbm.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_lgbm))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_lgbm)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_lgbm))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_lgbm)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_lgbm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b97426",
      "metadata": {
        "id": "f0b97426"
      },
      "outputs": [],
      "source": [
        "from skforecast.model_selection import grid_search_forecaster\n",
        "\n",
        "forecaster = ForecasterAutoreg(\n",
        "                regressor = LGBMRegressor(random_state=77),\n",
        "                lags      = 12\n",
        "             )\n",
        "\n",
        "# Lags used as predictors\n",
        "lags_grid = [12, 13, [1,2,9,12,13]]\n",
        "\n",
        "# Regressor's hyperparameters\n",
        "param_grid = {'n_estimators': [100, 300, 500],\n",
        "              'max_depth': [5, 10, 20, 30],\n",
        "              'learning_rate': [0.01, 0.1, 0.5, 1]}\n",
        "\n",
        "results_grid_lgbm = grid_search_forecaster(\n",
        "                        forecaster         = forecaster,\n",
        "                        y                  = df_train['passengers'],\n",
        "                        param_grid         = param_grid,\n",
        "                        lags_grid          = lags_grid,\n",
        "                        steps              = horizonte,\n",
        "                        refit              = False,\n",
        "                        metric             = 'mean_absolute_error',\n",
        "                        initial_train_size = int(len(df_train)*0.5),\n",
        "                        fixed_train_size   = False,\n",
        "                        return_best        = True,\n",
        "                        verbose            = False\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514d056c",
      "metadata": {
        "id": "514d056c"
      },
      "outputs": [],
      "source": [
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "mod_lgbm_grid = ForecasterAutoreg(\n",
        "                regressor = LGBMRegressor(n_estimators=100,\n",
        "                                         max_depth=30,\n",
        "                                         learning_rate=1,\n",
        "                                         random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_lgbm_grid.fit(y=df_train['passengers'])\n",
        "pred_lgbm_grid = mod_lgbm_grid.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_lgbm_grid))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_lgbm_grid)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_lgbm_grid))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_lgbm_grid)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_lgbm_grid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6c14d7",
      "metadata": {
        "id": "5c6c14d7"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_lgbm_grid, label='Predicción - LGBMRegressor (grid)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9d3cb6",
      "metadata": {
        "id": "eb9d3cb6"
      },
      "source": [
        "### CatBoost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc68a5f8",
      "metadata": {
        "id": "dc68a5f8"
      },
      "source": [
        "_CatBoost_ es un framework open-source de _gradient boosting_ que está diseñado para funcionar bien especialmente con características categóricas sin necesidad de codificación one-hot u otros pasos de preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb495e45",
      "metadata": {
        "id": "cb495e45"
      },
      "outputs": [],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99920d6f",
      "metadata": {
        "id": "99920d6f"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_cat = ForecasterAutoreg(\n",
        "                regressor = CatBoostRegressor(random_state=77, silent=True),\n",
        "                lags = 12\n",
        "                )\n",
        "\n",
        "mod_cat.fit(y=df_train['passengers'])\n",
        "pred_cat = mod_cat.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_cat))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_cat)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_cat))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_cat)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_cat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9467817",
      "metadata": {
        "id": "e9467817"
      },
      "outputs": [],
      "source": [
        "from skforecast.model_selection import grid_search_forecaster\n",
        "\n",
        "# Hyperparameter Grid search\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterAutoreg(\n",
        "                regressor = CatBoostRegressor(random_state=77, silent=True),\n",
        "                lags      = 12\n",
        "             )\n",
        "\n",
        "# Lags used as predictors\n",
        "lags_grid = [12, 13, [1,2,9,12,13]]\n",
        "\n",
        "# Regressor's hyperparameters\n",
        "param_grid = {'n_estimators': [100, 300, 500],\n",
        "              'max_depth': [5, 10],\n",
        "              'learning_rate': [0.01, 0.1, 1]}\n",
        "\n",
        "results_grid_cat = grid_search_forecaster(\n",
        "                        forecaster         = forecaster,\n",
        "                        y                  = df_train['passengers'],\n",
        "                        param_grid         = param_grid,\n",
        "                        lags_grid          = lags_grid,\n",
        "                        steps              = horizonte,\n",
        "                        refit              = False,\n",
        "                        metric             = 'mean_absolute_error',\n",
        "                        initial_train_size = int(len(df_train)*0.5),\n",
        "                        fixed_train_size   = False,\n",
        "                        return_best        = True,\n",
        "                        verbose            = False\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa861628",
      "metadata": {
        "id": "aa861628"
      },
      "outputs": [],
      "source": [
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "mod_cat_grid = ForecasterAutoreg(\n",
        "                regressor = CatBoostRegressor(n_estimators=300,\n",
        "                                              max_depth=5,\n",
        "                                              learning_rate=0.1,\n",
        "                                              random_state=77,\n",
        "                                              silent=True),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_cat_grid.fit(y=df_train['passengers'])\n",
        "pred_cat_grid = mod_cat_grid.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_cat_grid))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_cat_grid)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_cat_grid))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_cat_grid)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_cat_grid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59aa580a",
      "metadata": {
        "id": "59aa580a"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_cat_grid, label='Predicción - CatBoostRegressor (grid)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56bf929",
      "metadata": {
        "id": "e56bf929"
      },
      "source": [
        "### Bayesian search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f088b8",
      "metadata": {
        "id": "16f088b8"
      },
      "source": [
        "La búsqueda en _grid_ puede generar buenos resultados, especialmente cuando se reduce el rango de búsqueda. Sin embargo, **no se tienen en cuanta los resultados obtenidos** en cada experimento, lo que **impide centrar la búsqueda en las regiones de mayor interés** para evitar los cálculos innecesarios.\n",
        "\n",
        "Una alternativa es utilizar **métodos de optimización bayesianos** para buscar hiperparámetros. En términos generales, la optimización de hiperparámetros bayesianos consiste en crear un modelo probabilístico en el que la función objetivo es la métrica de validación del modelo (RMSE, AUC, precisión...). Con esta estrategia, **la búsqueda se redirige en cada iteración a las regiones de mayor interés**. El objetivo final es **reducir el número de combinaciones de hiperparámetros** con las que se evalúa el modelo, eligiendo solo los mejores candidatos.\n",
        "\n",
        "_Skforecast_ ofrece algunos motores de optimización bayesianos. Aquí usamos un ejempo el método **`Optuna`**. La búsqueda bayesiana solo se aplica a los hiperparámetros del modelo, y no a los retrasos, ya que se evalúan todos los _lags_ especificados por el usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff9d293",
      "metadata": {
        "id": "3ff9d293"
      },
      "outputs": [],
      "source": [
        "from skforecast.model_selection import  bayesian_search_forecaster\n",
        "\n",
        "forecaster = ForecasterAutoreg(\n",
        "                 regressor = XGBRegressor(random_state=77),\n",
        "                 lags      = 12\n",
        "             )\n",
        "\n",
        "# Lags\n",
        "lags_grid = [12, 13, [1,2,9,12,13]]\n",
        "\n",
        "# Parámetros del regresor\n",
        "def search_space(trial):\n",
        "    search_space  = {'n_estimators'     : trial.suggest_int('n_estimators', 100, 500, 100),\n",
        "                     'max_depth'        : trial.suggest_int('max_depth', 5, 30, 5)}\n",
        "    return search_space\n",
        "\n",
        "results, frozen_trial = bayesian_search_forecaster(\n",
        "                            forecaster            = forecaster,\n",
        "                            y                     = df_train['passengers'],\n",
        "                            lags_grid             = lags_grid,\n",
        "                            search_space          = search_space,\n",
        "                            steps                 = horizonte,\n",
        "                            metric                = 'mean_absolute_error',\n",
        "                            refit                 = True,\n",
        "                            initial_train_size    = int(len(df_train)*0.5),\n",
        "                            fixed_train_size      = True,\n",
        "                            n_trials              = 10,\n",
        "                            random_state          = 77,\n",
        "                            return_best           = True,\n",
        "                            verbose               = False,\n",
        "                            engine                = 'optuna',\n",
        "                            kwargs_create_study   = {},\n",
        "                            kwargs_study_optimize = {}\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2ecb7f",
      "metadata": {
        "id": "ac2ecb7f"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "mod_xgb_grid_opt = ForecasterAutoreg(\n",
        "                regressor = XGBRegressor(n_estimators=400,\n",
        "                                         max_depth=5,\n",
        "                                         random_state=77),\n",
        "                lags = [1,2,9,12,13]\n",
        "                )\n",
        "\n",
        "mod_xgb_grid_opt.fit(y=df_train['passengers'])\n",
        "pred_xgb_grid_opt = mod_xgb_grid_opt.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_test, pred_xgb_grid_opt))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_test, pred_xgb_grid_opt)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_test, pred_xgb_grid_opt))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_test, pred_xgb_grid_opt)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_test, pred_xgb_grid_opt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f124c3c3",
      "metadata": {
        "id": "f124c3c3"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_test, label='Test')\n",
        "plt.plot(pred_xgb_grid_opt, label='Predicción - XGBRegressor (grid con Optuna)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6afb571b",
      "metadata": {
        "id": "6afb571b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4ff48f",
      "metadata": {
        "id": "ba4ff48f"
      },
      "source": [
        "### **`Ejercicio 26`**\n",
        "\n",
        "Vamos a analizar los datos de **`Sunspots Dataset`** que son números promediados mensuales de **manchas solares desde 1749 hasta 1983**.  \n",
        "\n",
        "\n",
        "- Utilizamos el siguiente enlace para descargar los datos y crear una tabla tipo _DataFrame_ con ellos:\n",
        "  - 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'  \n",
        "  \n",
        "\n",
        "- Construimos la serie temporal, del modo que las fechas tipo `'1749-05-01'` formen los índices, y número de las manchas solares los valores de la serie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc03d007",
      "metadata": {
        "id": "fc03d007"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8383c4ca",
      "metadata": {
        "id": "8383c4ca"
      },
      "outputs": [],
      "source": [
        "# Solución 25.1.1\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Air Quality dataset\n",
        "df_spot = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv', index_col=False)\n",
        "df_spot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b52afa9",
      "metadata": {
        "id": "3b52afa9"
      },
      "outputs": [],
      "source": [
        "# Solución 25.1.2\n",
        "# Convertir la columna en fechas tipo DatetimeIndex\n",
        "df_spot['fechas'] = pd.to_datetime(df_spot['Month'], format='%Y-%m')\n",
        "\n",
        "# Convertir la columna de fechas a los índices del DataFrame\n",
        "df_spot.set_index('fechas', inplace=True)\n",
        "df_spot.index.freq = 'MS'\n",
        "\n",
        "# Remove the \"year\" and \"month\" columns\n",
        "df_spot.drop(columns='Month', inplace=True)\n",
        "\n",
        "df_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c38902",
      "metadata": {
        "id": "b2c38902"
      },
      "outputs": [],
      "source": [
        "# Visualizar el DataFrame creado con los datos de la serie temporal\n",
        "plt.plot(df_spot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7e926f",
      "metadata": {
        "id": "1a7e926f"
      },
      "outputs": [],
      "source": [
        "# Definir el periodo de prueba (horizonte de predicción)\n",
        "horizonte = 12  # La cantidad de puntos a predecir\n",
        "df_spot_test = df_spot.tail(horizonte)\n",
        "df_spot_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7a14b6",
      "metadata": {
        "id": "ee7a14b6"
      },
      "outputs": [],
      "source": [
        "# Filtrar la serie original para sacar el periodo de entrenamiento\n",
        "df_spot_train = df_spot[df_spot.index.isin(df_spot_test.index)==False]\n",
        "df_spot_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc0ee42",
      "metadata": {
        "id": "3dc0ee42"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 7\n",
        "plot_acf(df_spot, lags=50)\n",
        "plt.xticks(np.arange(50))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be2aca2",
      "metadata": {
        "id": "5be2aca2"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "plot_pacf(df_spot, lags=50, method='ywm')\n",
        "plt.xticks(np.arange(50))\n",
        "plt.ylim(-1.1,1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75587ef",
      "metadata": {
        "id": "e75587ef"
      },
      "source": [
        "Vemos que los retrasos con algo de autocorrelación parcial pueden llegar hasta el lag 34 aproximadamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198094dd",
      "metadata": {
        "id": "198094dd"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(10,301,50)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = RandomForestRegressor(n_estimators=hiper_param[i],\n",
        "                                                      random_state=77),\n",
        "                lags = 34\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_spot_train['Sunspots'])\n",
        "    test_r2[i] = r2_score(df_spot_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de lags podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (lags)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f154556",
      "metadata": {
        "id": "3f154556"
      },
      "outputs": [],
      "source": [
        "# Consideramos un rango para asignar el hiperparámetro\n",
        "hiper_param = np.arange(2,30)\n",
        "\n",
        "# Generamos previamente los vectores necesarios para ir calculando y guardando el rendimiento\n",
        "test_r2 = np.zeros(hiper_param.size)\n",
        "\n",
        "for i in range(hiper_param.size):\n",
        "    # Generamos un modelo para cada hiperparámetro\n",
        "    mod_bosque = ForecasterAutoreg(\n",
        "                    regressor = RandomForestRegressor(max_depth=hiper_param[i],\n",
        "                                                      n_estimators=110,\n",
        "                                                      random_state=77),\n",
        "                lags = 34\n",
        "                )\n",
        "\n",
        "    mod_bosque.fit(y=df_spot_train['Sunspots'])\n",
        "    test_r2[i] = r2_score(df_spot_test, mod_bosque.predict(steps=horizonte).round())\n",
        "\n",
        "print(\"El mejor valor de max_depth podría ser =\", hiper_param[np.argmax(test_r2)],\n",
        "      \" que consigue un R2 =\", max(test_r2))\n",
        "\n",
        "# Graficamos el R_cuadrado\n",
        "fig = plt.figure(figsize=(20,7))\n",
        "plt.plot(hiper_param, test_r2, linewidth=3, label='Test R^2')\n",
        "plt.plot(hiper_param[np.argmax(test_r2)], max(test_r2),\n",
        "        marker='o', color = \"red\", label=\"max R^2\")\n",
        "plt.xticks(hiper_param)\n",
        "plt.xlabel('Complejidad (max_depth)')\n",
        "plt.ylabel('R2')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56c6510",
      "metadata": {
        "id": "d56c6510"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "\n",
        "\n",
        "mod_spot_rf = ForecasterAutoreg(\n",
        "                regressor = RandomForestRegressor(n_estimators=110,\n",
        "                                                  max_depth=20,\n",
        "                                                  random_state=77),\n",
        "                lags = 34\n",
        "                )\n",
        "\n",
        "mod_spot_rf.fit(y=df_spot_train['Sunspots'])\n",
        "pred_spot_rf = mod_spot_rf.predict(steps=horizonte).round()\n",
        "\n",
        "# Métricas de evaluación del modelo\n",
        "print('Mean Absolute Error (MAE):', mean_absolute_error(df_spot_test, pred_spot_rf))\n",
        "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_spot_test, pred_spot_rf)*100)\n",
        "print('Mean Squared Error (MSE):', mean_squared_error(df_spot_test, pred_spot_rf))\n",
        "print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(df_spot_test, pred_spot_rf)))\n",
        "print('R^2 coefficient of determination:', r2_score(df_spot_test, pred_spot_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a77062",
      "metadata": {
        "id": "b3a77062"
      },
      "outputs": [],
      "source": [
        "plt.plot(df_spot_test, label='Test')\n",
        "plt.plot(pred_spot_rf, label='Predicción - RF (opt)')\n",
        "plt.title(\"Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2155c79",
      "metadata": {
        "id": "b2155c79"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504c0935",
      "metadata": {
        "id": "504c0935"
      },
      "source": [
        "### **`Ejercicio 26.1`**\n",
        "\n",
        "Vamos a intentar a modelar la serie temporal con el objetivo de predecir los valores mensuales del último año.\n",
        "\n",
        "**`26.1.1`** Realiza una búsqueda mediante _Compexity Curve_ para encontrar el número óptimo de retrasos, teniendo en cuenta los siguientes puntos:\n",
        "\n",
        "- Hasta `lag_34` inclusive\n",
        "- Modelo: **XGBoost**\n",
        "- `random_state=77`\n",
        "\n",
        "**`26.1.2`** Teniendo en cuenta el valor óptimo calculado en el paso anterior para los retrasos, realiza una búsqueda mediante _Compexity Curve_ para encontrar el número óptimo de **árboles**.\n",
        "\n",
        "**`26.1.3`** Teniendo en cuenta los valores óptimos calculados en los pasos anteriores, realiza una búsqueda mediante _Compexity Curve_ para encontrar el número óptimo de la **profundidad máxima**.\n",
        "\n",
        "**`26.1.4`** Construye un modelo con los hiperparámetros óptimos que hayas calculado y calcula las métricas de calidad del modelo y de tus predicciones:\n",
        "\n",
        "- Las métricas de \"_Out-of-sample performance_\": MAE, MAPE, MSE, RMSE y R2.\n",
        "\n",
        "**`26.1.5`** Saca la gráfica de \"_Datos reales vs. Predicción del modelo (Out-of-Sample forecasting)_\" y **analiza y compara los resultados** de este modelo con el último modelo contruido en la sesión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5293cca",
      "metadata": {
        "id": "d5293cca"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}